#!/usr/bin/env python3
"""
æ‰¹æ¬¡è¯„ä¼°æ¨¡å— - æ”¯æŒæ‰¹æ¬¡æäº¤å’Œå¹¶å‘å¤„ç†
"""

import os
import time
import yaml
import logging
from typing import List, Dict, Tuple, Optional, Any
from .optimization_engine import OptimizationCandidate

logger = logging.getLogger(__name__)

class BatchEvaluator:
    """æ‰¹æ¬¡è¯„ä¼°å™¨ï¼Œæ”¯æŒæ‰¹é‡æäº¤å’Œç®¡ç†Boltzä»»åŠ¡"""
    
    def __init__(self, boltz_client, scoring_system, batch_size: int = 4):
        self.boltz_client = boltz_client
        self.scoring_system = scoring_system
        self.batch_size = batch_size
        
    def evaluate_candidates_batch(self, 
                                candidates: List[OptimizationCandidate],
                                target_protein_yaml: str,
                                output_dir: str,
                                original_compound: str = None) -> List[OptimizationCandidate]:
        """
        æ‰¹æ¬¡è¯„ä¼°å€™é€‰åŒ–åˆç‰©
        
        Args:
            candidates: å¾…è¯„ä¼°çš„å€™é€‰åŒ–åˆç‰©åˆ—è¡¨
            target_protein_yaml: ç›®æ ‡è›‹ç™½è´¨é…ç½®YAMLè·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            original_compound: åŸå§‹åŒ–åˆç‰©SMILES
            
        Returns:
            æˆåŠŸè¯„ä¼°çš„å€™é€‰åŒ–åˆç‰©åˆ—è¡¨
        """
        evaluated_candidates = []
        
        # è¯»å–ç›®æ ‡è›‹ç™½è´¨é…ç½®
        with open(target_protein_yaml, 'r') as f:
            target_config = yaml.safe_load(f)
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•å’ŒCSVæ–‡ä»¶
        temp_config_dir = os.path.join(output_dir, "temp_configs")
        os.makedirs(temp_config_dir, exist_ok=True)
        
        csv_file = os.path.join(output_dir, "optimization_results.csv")
        self._initialize_csv_file(csv_file)
        
        logger.info(f"æ‰¹æ¬¡è¯„ä¼° {len(candidates)} ä¸ªå€™é€‰åŒ–åˆç‰©")
        logger.info(f"æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        logger.info(f"å®æ—¶ç»“æœä¿å­˜åˆ°: {csv_file}")
        
        # æŒ‰æ‰¹æ¬¡å¤„ç†å€™é€‰åŒ–åˆç‰©
        for batch_idx in range(0, len(candidates), self.batch_size):
            batch_end = min(batch_idx + self.batch_size, len(candidates))
            batch_candidates = candidates[batch_idx:batch_end]
            batch_num = batch_idx // self.batch_size + 1
            
            logger.info(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_num}: å€™é€‰åŒ–åˆç‰© {batch_idx + 1}-{batch_end}")
            
            # ç¬¬ä¸€é˜¶æ®µï¼šæäº¤æ•´ä¸ªæ‰¹æ¬¡
            batch_tasks = self._submit_batch(batch_candidates, target_config, csv_file, original_compound)
            
            if not batch_tasks:
                logger.warning(f"æ‰¹æ¬¡ {batch_num} æ²¡æœ‰æˆåŠŸæäº¤çš„ä»»åŠ¡")
                continue
            
            logger.info(f"æ‰¹æ¬¡ {batch_num} æäº¤äº† {len(batch_tasks)} ä¸ªä»»åŠ¡")
            
            # ç¬¬äºŒé˜¶æ®µï¼šç­‰å¾…æ‰¹æ¬¡å®Œæˆå¹¶å¤„ç†ç»“æœ
            batch_results = self._process_batch_results(batch_tasks, output_dir, csv_file, original_compound)
            evaluated_candidates.extend(batch_results)
            
            logger.info(f"âœ… æ‰¹æ¬¡ {batch_num} å®Œæˆï¼ŒæˆåŠŸè¯„ä¼° {len(batch_results)} ä¸ªå€™é€‰åŒ–åˆç‰©")
        
        logger.info(f"ğŸ‰ æ€»å…±æˆåŠŸè¯„ä¼° {len(evaluated_candidates)} ä¸ªå€™é€‰åŒ–åˆç‰©")
        return evaluated_candidates
    
    def _submit_batch(self, 
                     batch_candidates: List[OptimizationCandidate],
                     target_config: Dict,
                     csv_file: str,
                     original_compound: str) -> List[Tuple[OptimizationCandidate, str]]:
        """æäº¤ä¸€ä¸ªæ‰¹æ¬¡çš„å€™é€‰åŒ–åˆç‰©"""
        batch_tasks = []
        
        for candidate in batch_candidates:
            try:
                logger.info(f"ğŸ“¤ æäº¤å€™é€‰åŒ–åˆç‰©: {candidate.compound_id}")
                
                # åˆ›å»ºå€™é€‰åŒ–åˆç‰©çš„é…ç½®
                config_yaml = self._create_candidate_config_yaml(candidate, target_config)
                
                if not config_yaml:
                    logger.warning(f"âŒ åˆ›å»ºé…ç½®å¤±è´¥: {candidate.compound_id}")
                    self._write_csv_row(csv_file, candidate, original_compound, status="config_failed")
                    continue
                
                # æäº¤åˆ°Boltz-WebUI
                task_id = self.boltz_client.submit_optimization_job(
                    yaml_content=config_yaml,
                    job_name=f"opt_{candidate.compound_id}",
                    compound_smiles=candidate.smiles
                )
                
                if not task_id:
                    logger.warning(f"âŒ æäº¤å¤±è´¥: {candidate.compound_id}")
                    self._write_csv_row(csv_file, candidate, original_compound, status="submit_failed")
                    continue
                
                logger.info(f"âœ… æäº¤æˆåŠŸ: {candidate.compound_id} -> ä»»åŠ¡ {task_id}")
                batch_tasks.append((candidate, task_id))
                
            except Exception as e:
                logger.error(f"âŒ æäº¤é”™è¯¯ {candidate.compound_id}: {e}")
                self._write_csv_row(csv_file, candidate, original_compound, status="submit_error")
        
        return batch_tasks
    
    def _process_batch_results(self, 
                              batch_tasks: List[Tuple[OptimizationCandidate, str]],
                              output_dir: str,
                              csv_file: str,
                              original_compound: str) -> List[OptimizationCandidate]:
        """å¤„ç†æ‰¹æ¬¡ç»“æœ"""
        evaluated_candidates = []
        
        logger.info(f"â³ ç­‰å¾… {len(batch_tasks)} ä¸ªä»»åŠ¡å®Œæˆ...")
        
        for candidate, task_id in batch_tasks:
            try:
                logger.info(f"ğŸ” ç›‘æ§ä»»åŠ¡: {candidate.compound_id} ({task_id})")
                
                # ç­‰å¾…ä»»åŠ¡å®Œæˆ
                result = self.boltz_client.poll_job_status(task_id)
                
                if result and result.get('status') == 'completed':
                    logger.info(f"âœ… ä»»åŠ¡å®Œæˆ: {candidate.compound_id}")
                    
                    # ä¸‹è½½ç»“æœ
                    result_dir = os.path.join(output_dir, "results", candidate.compound_id)
                    try:
                        result_files = self.boltz_client.download_results(task_id, result_dir)
                        
                        if result_files:
                            # è§£æé¢„æµ‹ç»“æœ
                            prediction_results = self._parse_prediction_results(result_dir)
                            candidate.prediction_results = prediction_results
                            
                            # ç«‹å³è¯„åˆ†
                            try:
                                score = self.scoring_system.score_compound(
                                    smiles=candidate.smiles,
                                    boltz_results=candidate.prediction_results,
                                    reference_smiles=original_compound
                                )
                                candidate.scores = score
                                
                                # ç«‹å³å†™å…¥CSV
                                self._write_csv_row(csv_file, candidate, original_compound, 
                                                  status="completed", score=score, task_id=task_id)
                                
                                evaluated_candidates.append(candidate)
                                logger.info(f"ğŸ¯ {candidate.compound_id} è¯„åˆ†å®Œæˆ - åˆ†æ•°: {score.combined_score:.4f}")
                                
                            except Exception as e:
                                logger.error(f"âŒ è¯„åˆ†å¤±è´¥ {candidate.compound_id}: {e}")
                                self._write_csv_row(csv_file, candidate, original_compound, 
                                                  status="scoring_failed", task_id=task_id)
                        else:
                            logger.warning(f"âŒ ç»“æœä¸‹è½½å¤±è´¥: {candidate.compound_id}")
                            self._write_csv_row(csv_file, candidate, original_compound, 
                                              status="download_failed", task_id=task_id)
                    
                    except Exception as e:
                        logger.warning(f"âŒ ç»“æœå¤„ç†é”™è¯¯ {candidate.compound_id}: {e}")
                        self._write_csv_row(csv_file, candidate, original_compound, 
                                          status="download_error", task_id=task_id)
                else:
                    logger.warning(f"âŒ ä»»åŠ¡å¤±è´¥æˆ–è¶…æ—¶: {candidate.compound_id}")
                    error_msg = result.get('error', 'æœªçŸ¥é”™è¯¯') if result else 'è¶…æ—¶'
                    self._write_csv_row(csv_file, candidate, original_compound, 
                                      status=f"task_failed_{error_msg}", task_id=task_id)
                    
            except Exception as e:
                logger.error(f"âŒ ä»»åŠ¡å¤„ç†é”™è¯¯ {task_id}: {e}")
                self._write_csv_row(csv_file, candidate, original_compound, 
                                  status="task_error", task_id=task_id)
        
        return evaluated_candidates
    
    def _create_candidate_config_yaml(self, candidate: OptimizationCandidate, target_config: Dict) -> Optional[str]:
        """ä¸ºå€™é€‰åŒ–åˆç‰©åˆ›å»ºYAMLé…ç½®"""
        try:
            import copy
            config = copy.deepcopy(target_config)
            
            # æ·»åŠ å€™é€‰åŒ–åˆç‰©ä½œä¸ºé…ä½“
            ligand_id = "B"  # ä½¿ç”¨Boltzè¦æ±‚çš„ç®€å•å­—æ¯ID
            ligand_entry = {
                "ligand": {
                    "id": ligand_id,
                    "smiles": candidate.smiles
                }
            }
            
            # å°†é…ä½“æ·»åŠ åˆ°åºåˆ—ä¸­
            config["sequences"].append(ligand_entry)
            
            # è®¾ç½®äº²å’ŒåŠ›å±æ€§
            if "properties" not in config:
                config["properties"] = []
            
            config["properties"].append({
                "affinity": {
                    "binder": ligand_id
                }
            })
            
            # è½¬æ¢ä¸ºYAMLå­—ç¬¦ä¸²
            yaml_content = yaml.dump(config, default_flow_style=False, allow_unicode=True)
            return yaml_content
            
        except Exception as e:
            logger.error(f"åˆ›å»ºå€™é€‰é…ç½®å¤±è´¥ {candidate.compound_id}: {e}")
            return None
    
    def _parse_prediction_results(self, result_dir: str) -> Dict[str, Any]:
        """è§£æé¢„æµ‹ç»“æœ"""
        # è¿™é‡Œä½¿ç”¨ä¸optimization_engineç›¸åŒçš„è§£æé€»è¾‘
        results = {}
        
        # è§£æäº²å’ŒåŠ›æ•°æ®
        affinity_file = os.path.join(result_dir, 'affinity_data.json')
        if os.path.exists(affinity_file):
            import json
            with open(affinity_file, 'r') as f:
                affinity_data = json.load(f)
                results.update(affinity_data)
        
        # è§£æç½®ä¿¡åº¦æ•°æ®
        confidence_file = os.path.join(result_dir, 'confidence_data_model_0.json')
        if os.path.exists(confidence_file):
            import json
            with open(confidence_file, 'r') as f:
                confidence_data = json.load(f)
                results.update(confidence_data)
        
        return results
    
    def _initialize_csv_file(self, csv_file: str):
        """åˆå§‹åŒ–CSVæ–‡ä»¶"""
        import csv
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'timestamp', 'compound_id', 'original_smiles', 'optimized_smiles',
                'mmp_transformation', 'status', 'task_id', 'combined_score',
                'binding_affinity', 'drug_likeness', 'synthetic_accessibility',
                'novelty', 'stability', 'plddt', 'iptm', 'binding_probability',
                'ic50_um', 'molecular_weight', 'logp', 'lipinski_violations', 'qed_score'
            ])
    
    def _write_csv_row(self, csv_file: str, candidate: OptimizationCandidate, 
                      original_compound: str, status: str, 
                      score: Optional[Any] = None, task_id: str = None):
        """å†™å…¥CSVè¡Œ"""
        import csv
        from datetime import datetime
        
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # åŸºç¡€æ•°æ®
        row_data = {
            'timestamp': timestamp,
            'compound_id': candidate.compound_id,
            'original_smiles': original_compound or '',
            'optimized_smiles': candidate.smiles,
            'mmp_transformation': getattr(candidate, 'transformation', ''),
            'status': status,
            'task_id': task_id or '',
        }
        
        # è¯„åˆ†æ•°æ®
        if score:
            row_data.update({
                'combined_score': f"{score.combined_score:.4f}",
                'binding_affinity': f"{score.binding_affinity:.4f}",
                'drug_likeness': f"{score.drug_likeness:.4f}",
                'synthetic_accessibility': f"{score.synthetic_accessibility:.4f}",
                'novelty': f"{score.novelty:.4f}",
                'stability': f"{score.stability:.4f}",
                'plddt': f"{getattr(score, 'plddt', 0):.4f}",
                'iptm': f"{getattr(score, 'iptm', 0):.4f}",
                'binding_probability': f"{getattr(score, 'binding_probability', 0):.4f}",
                'ic50_um': f"{getattr(score, 'ic50_um', 0):.4f}",
                'molecular_weight': f"{getattr(score, 'molecular_weight', 0):.2f}",
                'logp': f"{getattr(score, 'logp', 0):.2f}",
                'lipinski_violations': f"{getattr(score, 'lipinski_violations', 0)}",
                'qed_score': f"{getattr(score, 'qed_score', 0):.4f}",
            })
        else:
            # ç©ºå€¼å¡«å……
            empty_fields = ['combined_score', 'binding_affinity', 'drug_likeness', 
                          'synthetic_accessibility', 'novelty', 'stability', 'plddt', 
                          'iptm', 'binding_probability', 'ic50_um', 'molecular_weight', 
                          'logp', 'lipinski_violations', 'qed_score']
            for field in empty_fields:
                row_data[field] = ''
        
        # å†™å…¥CSV
        with open(csv_file, 'a', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=row_data.keys())
            writer.writerow(row_data)
