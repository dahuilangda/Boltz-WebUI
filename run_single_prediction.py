# run_single_prediction.py
import sys
import os
import json
import tempfile
import shutil
import traceback
import yaml
import hashlib
import glob
import csv
import zipfile
import shlex
import requests
import time
import tarfile
import io
from pathlib import Path
from typing import Optional
import subprocess

sys.path.append(os.getcwd())
from boltz_wrapper import predict
from config import (
    MSA_SERVER_URL,
    MSA_SERVER_MODE,
    ALPHAFOLD3_DOCKER_IMAGE,
    ALPHAFOLD3_MODEL_DIR,
    ALPHAFOLD3_DATABASE_DIR,
    ALPHAFOLD3_DOCKER_EXTRA_ARGS,
)
from af3_adapter import (
    AF3Preparation,
    build_af3_fasta,
    build_af3_json,
    collect_chain_msa_paths,
    load_unpaired_msa,
    parse_yaml_for_af3,
    safe_filename,
    serialize_af3_json,
)

# MSA ç¼“å­˜é…ç½®
MSA_CACHE_CONFIG = {
    'cache_dir': '/tmp/boltz_msa_cache',
    'enable_cache': True
}

def get_sequence_hash(sequence: str) -> str:
    """è®¡ç®—åºåˆ—çš„MD5å“ˆå¸Œå€¼ä½œä¸ºç¼“å­˜é”®"""
    return hashlib.md5(sequence.encode('utf-8')).hexdigest()

def request_msa_from_server(sequence: str, timeout: int = 600) -> dict:
    """
    ä» ColabFold MSA æœåŠ¡å™¨è¯·æ±‚å¤šåºåˆ—æ¯”å¯¹
    
    Args:
        sequence: è›‹ç™½è´¨åºåˆ—ï¼ˆFASTA æ ¼å¼ï¼‰
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    Returns:
        åŒ…å« MSA ç»“æœçš„å­—å…¸ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å› None
    """
    try:
        print(f"ğŸ” æ­£åœ¨ä» MSA æœåŠ¡å™¨è¯·æ±‚å¤šåºåˆ—æ¯”å¯¹: {MSA_SERVER_URL}", file=sys.stderr)
        
        # å‡†å¤‡è¯·æ±‚æ•°æ®
        # ç¡®ä¿åºåˆ—æ˜¯ FASTA æ ¼å¼
        if not sequence.startswith('>'):
            sequence = f">query\n{sequence}"
        
        # ColabFold MSA æœåŠ¡å™¨ä½¿ç”¨ form data æ ¼å¼
        payload = {
            "q": sequence,
            "mode": MSA_SERVER_MODE
        }
        print(f"ğŸ“¦ MSA è¯·æ±‚å‚æ•°: mode={MSA_SERVER_MODE}", file=sys.stderr)
        
        # æäº¤æœç´¢ä»»åŠ¡
        submit_url = f"{MSA_SERVER_URL}/ticket/msa"
        print(f"ğŸ“¤ æäº¤ MSA æœç´¢ä»»åŠ¡åˆ°: {submit_url}", file=sys.stderr)
        
        response = requests.post(submit_url, data=payload, timeout=30)
        if response.status_code != 200:
            print(f"âŒ MSA ä»»åŠ¡æäº¤å¤±è´¥: {response.status_code} - {response.text}", file=sys.stderr)
            return None
        
        result = response.json()
        ticket_id = result.get("id")
        if not ticket_id:
            print(f"âŒ æœªè·å–åˆ°æœ‰æ•ˆçš„ä»»åŠ¡ ID: {result}", file=sys.stderr)
            return None
        
        print(f"âœ… MSA ä»»åŠ¡å·²æäº¤ï¼Œä»»åŠ¡ ID: {ticket_id}", file=sys.stderr)
        
        # è½®è¯¢ç»“æœ
        result_url = f"{MSA_SERVER_URL}/ticket/{ticket_id}"
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            try:
                print(f"â³ æ£€æŸ¥ MSA ä»»åŠ¡çŠ¶æ€...", file=sys.stderr)
                response = requests.get(result_url, timeout=30)
                
                if response.status_code == 200:
                    result_data = response.json()
                    if result_data.get("status") == "COMPLETE":
                        print(f"âœ… MSA æœç´¢å®Œæˆï¼Œè·å–åˆ°ç»“æœ", file=sys.stderr)
                        download_url = result_data.get("result_url") or f"{MSA_SERVER_URL}/result/download/{ticket_id}"
                        print(f"ğŸ“¥ ä¸‹è½½ MSA ç»“æœ: {download_url}", file=sys.stderr)
                        try:
                            download_response = requests.get(download_url, timeout=60)
                        except requests.exceptions.RequestException as download_error:
                            print(f"âŒ ä¸‹è½½ MSA ç»“æœè¯·æ±‚å¤±è´¥: {download_error}", file=sys.stderr)
                            return None
                        if download_response.status_code != 200:
                            print(
                                f"âŒ ä¸‹è½½ MSA ç»“æœå¤±è´¥: {download_response.status_code} - {download_response.text}",
                                file=sys.stderr,
                            )
                            return None

                        try:
                            tar_bytes = io.BytesIO(download_response.content)
                            with tarfile.open(fileobj=tar_bytes, mode="r:gz") as tar:
                                a3m_content = None
                                extracted_filename = None
                                for member in tar.getmembers():
                                    if member.name.lower().endswith(".a3m"):
                                        file_obj = tar.extractfile(member)
                                        if file_obj:
                                            a3m_content = file_obj.read().decode("utf-8")
                                            extracted_filename = member.name
                                            break

                            if not a3m_content:
                                print("âŒ æœªåœ¨ä¸‹è½½çš„ç»“æœä¸­æ‰¾åˆ° A3M æ–‡ä»¶", file=sys.stderr)
                                return None

                            print(f"âœ… æˆåŠŸæå– A3M æ–‡ä»¶: {extracted_filename}", file=sys.stderr)
                            entries = parse_a3m_content(a3m_content)
                            return {
                                "entries": entries,
                                "a3m_content": a3m_content,
                                "source": extracted_filename,
                                "ticket_id": ticket_id,
                            }
                        except tarfile.TarError as tar_error:
                            print(f"âŒ è§£æ MSA å‹ç¼©åŒ…å¤±è´¥: {tar_error}", file=sys.stderr)
                            return None
                    elif result_data.get("status") == "ERROR":
                        print(f"âŒ MSA æœç´¢å¤±è´¥: {result_data.get('error', 'æœªçŸ¥é”™è¯¯')}", file=sys.stderr)
                        print(
                            f"   â†³ æœåŠ¡å™¨è¿”å›: {json.dumps(result_data, ensure_ascii=False)}",
                            file=sys.stderr,
                        )
                        return None
                    else:
                        print(f"â³ MSA ä»»åŠ¡çŠ¶æ€: {result_data.get('status', 'PENDING')}", file=sys.stderr)
                elif response.status_code == 404:
                    print(f"â³ ä»»åŠ¡å°šæœªå®Œæˆæˆ–ä¸å­˜åœ¨", file=sys.stderr)
                else:
                    print(f"âš ï¸ æ£€æŸ¥çŠ¶æ€æ—¶å‡ºç°é”™è¯¯: {response.status_code}", file=sys.stderr)
                
            except requests.exceptions.RequestException as e:
                print(f"âš ï¸ æ£€æŸ¥çŠ¶æ€æ—¶ç½‘ç»œé”™è¯¯: {e}", file=sys.stderr)
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´å†æ¬¡æ£€æŸ¥
            time.sleep(10)
        
        print(f"â° MSA æœç´¢è¶…æ—¶ ({timeout}ç§’)", file=sys.stderr)
        return None
        
    except Exception as e:
        print(f"âŒ MSA æœåŠ¡å™¨è¯·æ±‚å¤±è´¥: {e}", file=sys.stderr)
        return None

def save_msa_result_to_file(msa_result: dict, output_path: str) -> bool:
    """
    å°† MSA ç»“æœä¿å­˜åˆ°æ–‡ä»¶
    
    Args:
        msa_result: MSA æœåŠ¡å™¨è¿”å›çš„ç»“æœ
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    
    Returns:
        æ˜¯å¦æˆåŠŸä¿å­˜
    """
    try:
        # æ ¹æ®ç»“æœæ ¼å¼ä¿å­˜ä¸º A3M æ–‡ä»¶
        if msa_result.get('a3m_content'):
            with open(output_path, 'w') as f:
                f.write(msa_result['a3m_content'])
            return True
        elif 'entries' in msa_result:
            with open(output_path, 'w') as f:
                for entry in msa_result['entries']:
                    name = entry.get('name', 'unknown')
                    sequence = entry.get('sequence', '')
                    if sequence:
                        f.write(f">{name}\n{sequence}\n")
            return True
        else:
            print(f"âŒ MSA ç»“æœæ ¼å¼ä¸æ”¯æŒ: {msa_result.keys()}", file=sys.stderr)
            return False
            
    except Exception as e:
        print(f"âŒ ä¿å­˜ MSA ç»“æœå¤±è´¥: {e}", file=sys.stderr)
        return False


def parse_a3m_content(a3m_content: str) -> list:
    """
    è§£æ A3M æ–‡ä»¶å†…å®¹ä¸ºåºåˆ—æ¡ç›®åˆ—è¡¨
    """
    entries = []
    current_name = None
    current_sequence_lines = []

    for line in a3m_content.splitlines():
        if line.startswith('>'):
            if current_name is not None:
                entries.append({
                    'name': current_name or 'unknown',
                    'sequence': ''.join(current_sequence_lines),
                })
            current_name = line[1:].strip()
            current_sequence_lines = []
        else:
            current_sequence_lines.append(line.strip())

    if current_name is not None:
        entries.append({
            'name': current_name or 'unknown',
            'sequence': ''.join(current_sequence_lines),
        })

    return entries

def generate_msa_for_sequences(yaml_content: str, temp_dir: str) -> bool:
    """
    ä¸º YAML ä¸­çš„è›‹ç™½è´¨åºåˆ—ç”Ÿæˆ MSA
    
    Args:
        yaml_content: YAML é…ç½®å†…å®¹
        temp_dir: ä¸´æ—¶ç›®å½•
    
    Returns:
        æ˜¯å¦æˆåŠŸç”Ÿæˆ MSA
    """
    try:
        print(f"ğŸ§¬ å¼€å§‹ä¸ºè›‹ç™½è´¨åºåˆ—ç”Ÿæˆ MSA", file=sys.stderr)
        
        # è§£æ YAML è·å–è›‹ç™½è´¨åºåˆ—
        yaml_data = yaml.safe_load(yaml_content)
        protein_sequences = {}
        
        for entity in yaml_data.get('sequences', []):
            if entity.get('protein', {}).get('id'):
                protein_id = entity['protein']['id']
                sequence = entity['protein'].get('sequence', '')
                if sequence:
                    protein_sequences[protein_id] = sequence
        
        if not protein_sequences:
            print("âŒ æœªæ‰¾åˆ°è›‹ç™½è´¨åºåˆ—ï¼Œè·³è¿‡ MSA ç”Ÿæˆ", file=sys.stderr)
            return False
        
        print(f"ğŸ” æ‰¾åˆ° {len(protein_sequences)} ä¸ªè›‹ç™½è´¨åºåˆ—éœ€è¦ç”Ÿæˆ MSA", file=sys.stderr)
        
        # ä¸ºæ¯ä¸ªè›‹ç™½è´¨åºåˆ—ç”Ÿæˆ MSA
        success_count = 0
        for protein_id, sequence in protein_sequences.items():
            print(f"ğŸ§¬ æ­£åœ¨ä¸ºè›‹ç™½è´¨ {protein_id} ç”Ÿæˆ MSA...", file=sys.stderr)
            
            # æ£€æŸ¥ä¸´æ—¶ç›®å½•ä¸­æ˜¯å¦å·²ç»å­˜åœ¨
            output_path = os.path.join(temp_dir, f"{protein_id}_msa.a3m")
            if os.path.exists(output_path):
                print(f"âœ… ä¸´æ—¶ç›®å½•ä¸­å·²å­˜åœ¨ MSA æ–‡ä»¶: {output_path}", file=sys.stderr)
                success_count += 1
                continue
            
            # æ£€æŸ¥ç¼“å­˜ï¼ˆç»Ÿä¸€ä½¿ç”¨ msa_ å‰ç¼€ï¼‰
            sequence_hash = get_sequence_hash(sequence)
            cache_dir = MSA_CACHE_CONFIG['cache_dir']
            cached_msa_path = os.path.join(cache_dir, f"msa_{sequence_hash}.a3m")
            
            if MSA_CACHE_CONFIG['enable_cache'] and os.path.exists(cached_msa_path):
                print(f"âœ… æ‰¾åˆ°ç¼“å­˜çš„ MSA æ–‡ä»¶: {cached_msa_path}", file=sys.stderr)
                # å¤åˆ¶åˆ°ä¸´æ—¶ç›®å½•
                shutil.copy2(cached_msa_path, output_path)
                success_count += 1
                continue
            
            # ä»æœåŠ¡å™¨è¯·æ±‚ MSA
            msa_result = request_msa_from_server(sequence)
            if msa_result:
                # ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
                if save_msa_result_to_file(msa_result, output_path):
                    success_count += 1
                    
                    # ç¼“å­˜ç»“æœï¼ˆç»Ÿä¸€ä½¿ç”¨ msa_ å‰ç¼€ï¼‰
                    if MSA_CACHE_CONFIG['enable_cache']:
                        os.makedirs(cache_dir, exist_ok=True)
                        shutil.copy2(output_path, cached_msa_path)
                        print(f"ğŸ’¾ MSA ç»“æœå·²ç¼“å­˜: {cached_msa_path}", file=sys.stderr)
                else:
                    print(f"âŒ ä¿å­˜ MSA æ–‡ä»¶å¤±è´¥: {protein_id}", file=sys.stderr)
            else:
                print(f"âŒ è·å– MSA å¤±è´¥: {protein_id}", file=sys.stderr)
        
        print(f"âœ… MSA ç”Ÿæˆå®Œæˆ: {success_count}/{len(protein_sequences)} ä¸ªæˆåŠŸ", file=sys.stderr)
        return success_count > 0
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆ MSA æ—¶å‡ºç°é”™è¯¯: {e}", file=sys.stderr)
        return False

def cache_msa_files_from_temp_dir(temp_dir: str, yaml_content: str):
    """
    ä»ä¸´æ—¶ç›®å½•ä¸­ç¼“å­˜ç”Ÿæˆçš„MSAæ–‡ä»¶
    æ”¯æŒä»colabfold serverç”Ÿæˆçš„CSVæ ¼å¼MSAæ–‡ä»¶
    ä¸ºæ¯ä¸ªè›‹ç™½è´¨ç»„åˆ†å•ç‹¬ç¼“å­˜MSAï¼Œé€‚ç”¨äºç»“æ„é¢„æµ‹å’Œåˆ†å­è®¾è®¡
    """
    if not MSA_CACHE_CONFIG['enable_cache']:
        return
    
    try:
        # è§£æYAMLè·å–è›‹ç™½è´¨åºåˆ—
        yaml_data = yaml.safe_load(yaml_content)
        protein_sequences = {}
        
        # æå–æ‰€æœ‰è›‹ç™½è´¨åºåˆ—ï¼ˆæ”¯æŒç»“æ„é¢„æµ‹å’Œåˆ†å­è®¾è®¡ï¼‰
        for entity in yaml_data.get('sequences', []):
            if entity.get('protein', {}).get('id'):
                protein_id = entity['protein']['id']
                sequence = entity['protein'].get('sequence', '')
                if sequence:
                    protein_sequences[protein_id] = sequence
        
        if not protein_sequences:
            print("æœªæ‰¾åˆ°è›‹ç™½è´¨åºåˆ—ï¼Œè·³è¿‡MSAç¼“å­˜", file=sys.stderr)
            return
        
        print(f"éœ€è¦ç¼“å­˜çš„è›‹ç™½è´¨ç»„åˆ†: {list(protein_sequences.keys())}", file=sys.stderr)
        
        # è®¾ç½®ç¼“å­˜ç›®å½•
        cache_dir = MSA_CACHE_CONFIG['cache_dir']
        os.makedirs(cache_dir, exist_ok=True)
        
        # é€’å½’æœç´¢ä¸´æ—¶ç›®å½•ä¸­çš„MSAæ–‡ä»¶
        print(f"é€’å½’æœç´¢ä¸´æ—¶ç›®å½•ä¸­çš„MSAæ–‡ä»¶: {temp_dir}", file=sys.stderr)
        
        # ä¸ºæ¯ä¸ªè›‹ç™½è´¨ç»„åˆ†å•ç‹¬æŸ¥æ‰¾å¯¹åº”çš„MSAæ–‡ä»¶
        protein_msa_map = {}  # protein_id -> [msa_files]
        
        # æœç´¢æ‰€æœ‰MSAæ–‡ä»¶
        all_msa_files = []
        for root, dirs, files in os.walk(temp_dir):
            for file in files:
                if file.endswith('.csv') or file.endswith('.a3m'):
                    file_path = os.path.join(root, file)
                    all_msa_files.append(file_path)
        
        if not all_msa_files:
            print(f"åœ¨ä¸´æ—¶ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•MSAæ–‡ä»¶: {temp_dir}", file=sys.stderr)
            return
        
        print(f"æ‰¾åˆ° {len(all_msa_files)} ä¸ªMSAæ–‡ä»¶: {[os.path.basename(f) for f in all_msa_files]}", file=sys.stderr)
        
        # ä¸ºæ¯ä¸ªè›‹ç™½è´¨ç»„åˆ†åŒ¹é…å¯¹åº”çš„MSAæ–‡ä»¶
        for protein_id in protein_sequences.keys():
            protein_msa_map[protein_id] = []
            
            for msa_file in all_msa_files:
                filename = os.path.basename(msa_file)
                
                # ç²¾ç¡®åŒ¹é…ï¼šæ–‡ä»¶ååŒ…å«protein ID
                if protein_id.lower() in filename.lower():
                    protein_msa_map[protein_id].append(msa_file)
                    continue
                    
                # ç´¢å¼•åŒ¹é…ï¼šå¦‚æœprotein_idæ˜¯å­—æ¯ï¼Œå°è¯•åŒ¹é…å¯¹åº”çš„æ•°å­—ç´¢å¼•
                # ä¾‹å¦‚ï¼šprotein A -> _0.csv, protein B -> _1.csv
                if len(protein_id) == 1 and protein_id.isalpha():
                    protein_index = ord(protein_id.upper()) - ord('A')
                    if f"_{protein_index}." in filename:
                        protein_msa_map[protein_id].append(msa_file)
                        continue
                
                # é€šç”¨åŒ¹é…ï¼šå¦‚æœåªæœ‰ä¸€ä¸ªè›‹ç™½è´¨ç»„åˆ†ï¼Œä½¿ç”¨é€šç”¨MSAæ–‡ä»¶
                if len(protein_sequences) == 1 and any(pattern in filename.lower() for pattern in ['msa', '_0.csv', '_0.a3m']):
                    protein_msa_map[protein_id].append(msa_file)
        
        # å¤„ç†æ¯ä¸ªè›‹ç™½è´¨ç»„åˆ†çš„MSAæ–‡ä»¶
        cached_count = 0
        for protein_id, msa_files in protein_msa_map.items():
            if not msa_files:
                print(f"âŒ è›‹ç™½è´¨ç»„åˆ† {protein_id} æœªæ‰¾åˆ°å¯¹åº”çš„MSAæ–‡ä»¶", file=sys.stderr)
                continue
                
            print(f"ğŸ” å¤„ç†è›‹ç™½è´¨ç»„åˆ† {protein_id} çš„ {len(msa_files)} ä¸ªMSAæ–‡ä»¶", file=sys.stderr)
            
            for msa_file in msa_files:
                if cache_single_protein_msa(protein_id, protein_sequences[protein_id], msa_file, cache_dir):
                    cached_count += 1
                    break  # æˆåŠŸç¼“å­˜ä¸€ä¸ªå°±å¤Ÿäº†
        
        print(f"âœ… MSAç¼“å­˜å®Œæˆï¼ŒæˆåŠŸç¼“å­˜ {cached_count}/{len(protein_sequences)} ä¸ªè›‹ç™½è´¨ç»„åˆ†", file=sys.stderr)
                
    except Exception as e:
        print(f"âŒ ç¼“å­˜MSAæ–‡ä»¶å¤±è´¥: {e}", file=sys.stderr)

def cache_single_protein_msa(protein_id: str, protein_sequence: str, msa_file: str, cache_dir: str) -> bool:
    """
    ä¸ºå•ä¸ªè›‹ç™½è´¨ç»„åˆ†ç¼“å­˜MSAæ–‡ä»¶
    è¿”å›æ˜¯å¦æˆåŠŸç¼“å­˜
    """
    try:
        filename = os.path.basename(msa_file)
        file_ext = os.path.splitext(filename)[1].lower()
        
        print(f"  ğŸ“‚ å¤„ç†MSAæ–‡ä»¶: {filename}", file=sys.stderr)
        
        if file_ext == '.csv':
            # å¤„ç†CSVæ ¼å¼çš„MSAæ–‡ä»¶ï¼ˆæ¥è‡ªcolabfold serverï¼‰
            with open(msa_file, 'r') as f:
                reader = csv.reader(f)
                header = next(reader, None)
                if header and len(header) >= 2 and 'sequence' in header:
                    sequences = []
                    for row in reader:
                        if len(row) >= 2 and row[1]:
                            sequences.append(row[1])
                    
                    if sequences:
                        # ç¬¬ä¸€ä¸ªåºåˆ—é€šå¸¸æ˜¯æŸ¥è¯¢åºåˆ—
                        query_sequence = sequences[0]
                        print(f"    ä»CSVæå–çš„æŸ¥è¯¢åºåˆ—: {query_sequence[:50]}...", file=sys.stderr)
                        
                        # éªŒè¯åºåˆ—æ˜¯å¦åŒ¹é…
                        if is_sequence_match(protein_sequence, query_sequence):
                            # è½¬æ¢CSVæ ¼å¼åˆ°A3Mæ ¼å¼
                            a3m_content = f">{protein_id}\n{query_sequence}\n"
                            for i, seq in enumerate(sequences[1:], 1):
                                a3m_content += f">seq_{i}\n{seq}\n"
                            
                            # ç¼“å­˜è½¬æ¢åçš„A3Mæ–‡ä»¶
                            seq_hash = get_sequence_hash(protein_sequence)
                            cache_path = os.path.join(cache_dir, f"msa_{seq_hash}.a3m")
                            with open(cache_path, 'w') as cache_file:
                                cache_file.write(a3m_content)
                            print(f"    âœ… æˆåŠŸç¼“å­˜è›‹ç™½è´¨ç»„åˆ† {protein_id} çš„MSA (ä»CSVè½¬æ¢): {cache_path}", file=sys.stderr)
                            print(f"       åºåˆ—å“ˆå¸Œ: {seq_hash}", file=sys.stderr)
                            print(f"       MSAåºåˆ—æ•°: {len(sequences)}", file=sys.stderr)
                            return True
                        else:
                            print(f"    âŒ CSVæ–‡ä»¶ä¸­çš„æŸ¥è¯¢åºåˆ—ä¸è›‹ç™½è´¨ç»„åˆ† {protein_id} ä¸åŒ¹é…", file=sys.stderr)
                            return False
        
        elif file_ext == '.a3m':
            # å¤„ç†A3Mæ ¼å¼çš„MSAæ–‡ä»¶
            with open(msa_file, 'r') as f:
                msa_content = f.read()
            
            # ä»MSAå†…å®¹ä¸­æå–æŸ¥è¯¢åºåˆ—ï¼ˆç¬¬ä¸€ä¸ªåºåˆ—ï¼‰
            lines = msa_content.strip().split('\n')
            if len(lines) >= 2 and lines[0].startswith('>'):
                query_sequence = lines[1]
                
                # éªŒè¯åºåˆ—æ˜¯å¦åŒ¹é…
                if is_sequence_match(protein_sequence, query_sequence):
                    # ç¼“å­˜MSAæ–‡ä»¶
                    seq_hash = get_sequence_hash(protein_sequence)
                    cache_path = os.path.join(cache_dir, f"msa_{seq_hash}.a3m")
                    shutil.copy2(msa_file, cache_path)
                    print(f"    âœ… æˆåŠŸç¼“å­˜è›‹ç™½è´¨ç»„åˆ† {protein_id} çš„MSA: {cache_path}", file=sys.stderr)
                    print(f"       åºåˆ—å“ˆå¸Œ: {seq_hash}", file=sys.stderr)
                    return True
                else:
                    print(f"    âŒ A3Mæ–‡ä»¶ä¸­çš„æŸ¥è¯¢åºåˆ—ä¸è›‹ç™½è´¨ç»„åˆ† {protein_id} ä¸åŒ¹é…", file=sys.stderr)
                    return False
        
        return False
        
    except Exception as e:
        print(f"    âŒ å¤„ç†è›‹ç™½è´¨ç»„åˆ† {protein_id} çš„MSAæ–‡ä»¶å¤±è´¥ {msa_file}: {e}", file=sys.stderr)
        return False

def is_sequence_match(protein_sequence: str, query_sequence: str) -> bool:
    """
    æ£€æŸ¥è›‹ç™½è´¨åºåˆ—å’ŒæŸ¥è¯¢åºåˆ—æ˜¯å¦åŒ¹é…
    æ”¯æŒå®Œå…¨åŒ¹é…ã€å®¹é”™åŒ¹é…å’Œç›¸ä¼¼åº¦åŒ¹é…
    """
    # å®Œå…¨åŒ¹é…
    if protein_sequence == query_sequence:
        return True
    
    # å®¹é”™åŒ¹é…ï¼šå»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦åæ¯”è¾ƒ
    clean_protein = protein_sequence.replace('-', '').replace(' ', '').upper()
    clean_query = query_sequence.replace('-', '').replace(' ', '').upper()
    if clean_protein == clean_query:
        return True
    
    # å­åºåˆ—åŒ¹é…ï¼šæŸ¥è¯¢åºåˆ—å¯èƒ½æ˜¯è›‹ç™½è´¨åºåˆ—çš„ä¸€éƒ¨åˆ†
    if clean_query in clean_protein or clean_protein in clean_query:
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = len(set(clean_query) & set(clean_protein)) / max(len(clean_query), len(clean_protein))
        if similarity > 0.8:  # 80%ç›¸ä¼¼åº¦é˜ˆå€¼
            return True
    
    return False

def find_results_dir(base_dir: str) -> str:
    result_path = None
    max_depth = -1
    for root, dirs, files in os.walk(base_dir):
        if any(f.endswith((".cif")) for f in files):
            depth = root.count(os.sep)
            if depth > max_depth:
                max_depth = depth
                result_path = root

    if result_path:
        print(f"Found results in directory: {result_path}", file=sys.stderr)
        return result_path

    raise FileNotFoundError(f"Could not find any directory containing result files within the base directory {base_dir}")

def get_cached_a3m_files(yaml_content: str) -> list:
    """
    è·å–ä¸å½“å‰é¢„æµ‹ä»»åŠ¡ç›¸å…³çš„a3mç¼“å­˜æ–‡ä»¶
    è¿”å›ç¼“å­˜æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    cached_a3m_files = []
    
    if not MSA_CACHE_CONFIG['enable_cache']:
        return cached_a3m_files
    
    try:
        # è§£æYAMLè·å–è›‹ç™½è´¨åºåˆ—
        yaml_data = yaml.safe_load(yaml_content)
        protein_sequences = {}
        
        # æå–æ‰€æœ‰è›‹ç™½è´¨åºåˆ—
        for entity in yaml_data.get('sequences', []):
            if entity.get('protein', {}).get('id'):
                protein_id = entity['protein']['id']
                sequence = entity['protein'].get('sequence', '')
                if sequence:
                    protein_sequences[protein_id] = sequence
        
        if not protein_sequences:
            print("æœªæ‰¾åˆ°è›‹ç™½è´¨åºåˆ—ï¼Œè·³è¿‡a3mæ–‡ä»¶æ”¶é›†", file=sys.stderr)
            return cached_a3m_files
        
        cache_dir = MSA_CACHE_CONFIG['cache_dir']
        if not os.path.exists(cache_dir):
            return cached_a3m_files
        
        print(f"æŸ¥æ‰¾ç¼“å­˜çš„a3mæ–‡ä»¶ï¼Œè›‹ç™½è´¨ç»„åˆ†: {list(protein_sequences.keys())}", file=sys.stderr)
        
        # ä¸ºæ¯ä¸ªè›‹ç™½è´¨åºåˆ—æŸ¥æ‰¾å¯¹åº”çš„ç¼“å­˜æ–‡ä»¶
        for protein_id, sequence in protein_sequences.items():
            seq_hash = get_sequence_hash(sequence)
            cache_file_path = os.path.join(cache_dir, f"msa_{seq_hash}.a3m")
            
            if os.path.exists(cache_file_path):
                cached_a3m_files.append({
                    'path': cache_file_path,
                    'protein_id': protein_id,
                    'filename': f"{protein_id}_msa.a3m"
                })
                print(f"æ‰¾åˆ°ç¼“å­˜æ–‡ä»¶: {protein_id} -> {cache_file_path}", file=sys.stderr)
        
        print(f"æ€»å…±æ‰¾åˆ° {len(cached_a3m_files)} ä¸ªa3mç¼“å­˜æ–‡ä»¶", file=sys.stderr)
        
    except Exception as e:
        print(f"è·å–a3mç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}", file=sys.stderr)
    
    return cached_a3m_files

def create_archive_with_a3m(output_archive_path: str, output_directory_path: str, yaml_content: str):
    """
    åˆ›å»ºåŒ…å«é¢„æµ‹ç»“æœå’Œa3mç¼“å­˜æ–‡ä»¶çš„zipå½’æ¡£
    """
    try:
        # è·å–ç›¸å…³çš„a3mç¼“å­˜æ–‡ä»¶
        cached_a3m_files = get_cached_a3m_files(yaml_content)
        
        # åˆ›å»ºzipæ–‡ä»¶
        with zipfile.ZipFile(output_archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            # æ·»åŠ é¢„æµ‹ç»“æœæ–‡ä»¶
            for root, dirs, files in os.walk(output_directory_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œä¿æŒç›®å½•ç»“æ„
                    arcname = os.path.relpath(file_path, output_directory_path)
                    zipf.write(file_path, arcname)
                    print(f"æ·»åŠ ç»“æœæ–‡ä»¶: {arcname}", file=sys.stderr)
            
            # æ·»åŠ a3mç¼“å­˜æ–‡ä»¶
            if cached_a3m_files:
                # åœ¨zipä¸­åˆ›å»ºmsaç›®å½•
                for a3m_info in cached_a3m_files:
                    cache_file_path = a3m_info['path']
                    filename = a3m_info['filename']
                    # å°†a3mæ–‡ä»¶æ”¾åœ¨msaå­ç›®å½•ä¸­
                    arcname = f"msa/{filename}"
                    zipf.write(cache_file_path, arcname)
                    print(f"æ·»åŠ a3mç¼“å­˜æ–‡ä»¶: {arcname}", file=sys.stderr)
                
                print(f"âœ… æˆåŠŸæ·»åŠ  {len(cached_a3m_files)} ä¸ªa3mç¼“å­˜æ–‡ä»¶åˆ°zipå½’æ¡£", file=sys.stderr)
            else:
                print("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³çš„a3mç¼“å­˜æ–‡ä»¶", file=sys.stderr)
        
        print(f"âœ… å½’æ¡£åˆ›å»ºå®Œæˆ: {output_archive_path}", file=sys.stderr)
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºåŒ…å«a3mæ–‡ä»¶çš„å½’æ¡£å¤±è´¥: {e}", file=sys.stderr)
        # å¦‚æœå¤±è´¥ï¼Œå›é€€åˆ°åŸæ¥çš„æ–¹å¼
        archive_base_name = output_archive_path.rsplit('.', 1)[0]
        created_archive_path = shutil.make_archive(
            base_name=archive_base_name,
            format='zip',
            root_dir=output_directory_path
        )
        print(f"å›é€€åˆ°æ ‡å‡†å½’æ¡£æ–¹å¼: {created_archive_path}", file=sys.stderr)


def create_af3_archive(
    output_archive_path: str,
    fasta_content: str,
    af3_json: dict,
    chain_msa_paths: dict,
    yaml_content: str,
    prep: AF3Preparation,
    af3_output_dir: Optional[str] = None,
) -> None:
    """
    Create an archive containing AF3-compatible assets (FASTA, JSON, and MSAs).
    """
    try:
        with zipfile.ZipFile(output_archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            zipf.writestr(f"af3/{prep.jobname}_input.fasta", fasta_content)
            zipf.writestr(f"af3/{prep.jobname}_input.json", serialize_af3_json(af3_json))
            zipf.writestr("af3/input.yaml", yaml_content)

            metadata = {
                "jobname": prep.jobname,
                "chain_labels": prep.header_labels,
                "sequence_cardinality": prep.query_sequences_cardinality,
                "chain_id_label_map": prep.chain_id_label_map,
            }
            zipf.writestr("af3/metadata.json", json.dumps(metadata, indent=2, ensure_ascii=False))

            if chain_msa_paths:
                for chain_id, path in chain_msa_paths.items():
                    if not path or not os.path.exists(path):
                        continue
                    arcname = f"af3/msa/{safe_filename(chain_id)}.a3m"
                    zipf.write(path, arcname)
                    print(f"æ·»åŠ AF3 MSAæ–‡ä»¶: {arcname}", file=sys.stderr)
            else:
                print("âš ï¸ æœªæ‰¾åˆ°AF3æ‰€éœ€çš„MSAæ–‡ä»¶ï¼ŒJSONä¸­å°†ç•™ç©º", file=sys.stderr)

            output_files_added = False
            if af3_output_dir and os.path.isdir(af3_output_dir):
                for root, _, files in os.walk(af3_output_dir):
                    for file in files:
                        file_path = os.path.join(root, file)
                        arcname = os.path.relpath(file_path, af3_output_dir)
                        arcname = os.path.join("af3/output", arcname)
                        zipf.write(file_path, arcname)
                        print(f"æ·»åŠ AF3è¾“å‡ºæ–‡ä»¶: {arcname}", file=sys.stderr)
                        output_files_added = True
            if not output_files_added:
                print("â„¹ï¸ AF3è¾“å‡ºç›®å½•ä¸ºç©ºæˆ–ç¼ºå¤±ï¼Œä»…ä¿ç•™è¾“å…¥æ–‡ä»¶", file=sys.stderr)

            instructions = (
                "AlphaFold3 input assets generated by Boltz-WebUI.\n"
                "Files included:\n"
                " - af3_input.fasta / af3_input.json: ready for AlphaFold3 jobs\n"
                " - msa directory: cached MSAs per chain (if available)\n"
                " - input.yaml: original request payload\n"
                " - output/: files produced by AlphaFold3 (if the docker run succeeded)\n"
                "\n"
                "Upload the JSON file to AlphaFold3 alongside the FASTA sequence.\n"
            )
            zipf.writestr("af3/README.txt", instructions)

        print(f"âœ… AF3 å½’æ¡£åˆ›å»ºå®Œæˆ: {output_archive_path}", file=sys.stderr)
    except Exception as e:
        raise RuntimeError(f"Failed to create AF3 archive: {e}") from e


def run_boltz_backend(
    temp_dir: str,
    yaml_content: str,
    output_archive_path: str,
    predict_args: dict,
    model_name: Optional[str],
) -> None:
    tmp_yaml_path = os.path.join(temp_dir, 'data.yaml')
    with open(tmp_yaml_path, 'w') as tmp_yaml:
        tmp_yaml.write(yaml_content)

    cli_args = dict(predict_args)
    if model_name:
        cli_args['model'] = model_name
        print(f"DEBUG: Using model: {model_name}", file=sys.stderr)

    cli_args['data'] = tmp_yaml_path
    cli_args['out_dir'] = temp_dir

    if MSA_SERVER_URL and MSA_SERVER_URL != "":
        print(f"ğŸ§¬ å¼€å§‹ä½¿ç”¨ MSA æœåŠ¡å™¨ç”Ÿæˆå¤šåºåˆ—æ¯”å¯¹: {MSA_SERVER_URL}", file=sys.stderr)
        msa_generated = generate_msa_for_sequences(yaml_content, temp_dir)
        if msa_generated:
            print(f"âœ… MSA ç”ŸæˆæˆåŠŸï¼Œå°†ç”¨äºç»“æ„é¢„æµ‹", file=sys.stderr)
        else:
            print(f"âš ï¸ MSA ç”Ÿæˆå¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤æ–¹æ³•è¿›è¡Œé¢„æµ‹", file=sys.stderr)
    else:
        print(f"â„¹ï¸ æœªé…ç½® MSA æœåŠ¡å™¨ï¼Œè·³è¿‡ MSA ç”Ÿæˆ", file=sys.stderr)

    POSITIONAL_KEYS = ['data']
    cmd_positional = []
    cmd_options = []

    for key, value in cli_args.items():
        if key in POSITIONAL_KEYS:
            cmd_positional.append(str(value))
        else:
            if value is None:
                continue
            if isinstance(value, bool):
                if value:
                    cmd_options.append(f'--{key}')
            else:
                cmd_options.append(f'--{key}')
                cmd_options.append(str(value))

    cmd_args = cmd_positional + cmd_options

    print(f"DEBUG: Invoking predict with args: {cmd_args}", file=sys.stderr)
    predict.main(args=cmd_args, standalone_mode=False)

    cache_msa_files_from_temp_dir(temp_dir, yaml_content)

    output_directory_path = find_results_dir(temp_dir)
    if not os.listdir(output_directory_path):
        raise NotADirectoryError(
            f"Prediction result directory was found but is empty: {output_directory_path}"
        )

    create_archive_with_a3m(output_archive_path, output_directory_path, yaml_content)


def run_alphafold3_backend(
    temp_dir: str,
    yaml_content: str,
    output_archive_path: str,
    use_msa_server: bool,
) -> None:
    print("ğŸš€ Using AlphaFold3 backend (AF3 input preparation)", file=sys.stderr)

    if use_msa_server and MSA_SERVER_URL and MSA_SERVER_URL != "":
        print(f"ğŸ§¬ å¼€å§‹ä½¿ç”¨ MSA æœåŠ¡å™¨ç”Ÿæˆå¤šåºåˆ—æ¯”å¯¹: {MSA_SERVER_URL}", file=sys.stderr)
        msa_generated = generate_msa_for_sequences(yaml_content, temp_dir)
        if msa_generated:
            print(f"âœ… MSA ç”ŸæˆæˆåŠŸï¼Œå°†ç”¨äºAF3è¾“å…¥", file=sys.stderr)
        else:
            print(f"âš ï¸ æœªèƒ½è·å–MSAï¼ŒAF3 JSONå°†å«ç©ºMSAå­—æ®µ", file=sys.stderr)
    else:
        print("â„¹ï¸ æœªé…ç½® MSA æœåŠ¡å™¨æˆ–æœªè¯·æ±‚ä½¿ç”¨ï¼Œå°†å°è¯•ä½¿ç”¨ç¼“å­˜çš„MSA", file=sys.stderr)

    prep = parse_yaml_for_af3(yaml_content)
    cache_dir = MSA_CACHE_CONFIG['cache_dir'] if MSA_CACHE_CONFIG['enable_cache'] else None
    chain_msa_paths = collect_chain_msa_paths(prep, temp_dir, cache_dir)
    unpaired_msa = load_unpaired_msa(prep, chain_msa_paths)
    fasta_content = build_af3_fasta(prep)
    af3_json = build_af3_json(prep, unpaired_msa)

    cache_msa_files_from_temp_dir(temp_dir, yaml_content)

    af3_input_dir = os.path.join(temp_dir, "af3_input")
    af3_output_dir = os.path.join(temp_dir, "af3_output")
    os.makedirs(af3_input_dir, exist_ok=True)
    os.makedirs(af3_output_dir, exist_ok=True)

    fasta_path = os.path.join(af3_input_dir, f"{prep.jobname}_input.fasta")
    json_path = os.path.join(af3_input_dir, "fold_input.json")

    with open(fasta_path, "w") as fasta_file:
        fasta_file.write(fasta_content)
    with open(json_path, "w") as json_file:
        json.dump(af3_json, json_file, indent=2, ensure_ascii=False)

    model_dir = ALPHAFOLD3_MODEL_DIR
    database_dir = ALPHAFOLD3_DATABASE_DIR
    image = ALPHAFOLD3_DOCKER_IMAGE or "alphafold3"
    extra_args = shlex.split(ALPHAFOLD3_DOCKER_EXTRA_ARGS) if ALPHAFOLD3_DOCKER_EXTRA_ARGS else []

    if not model_dir or not os.path.isdir(model_dir):
        raise FileNotFoundError("ALPHAFOLD3_MODEL_DIR æœªé…ç½®æˆ–ç›®å½•ä¸å­˜åœ¨ï¼Œæ— æ³•è¿è¡Œ AlphaFold3 å®¹å™¨ã€‚")
    if not database_dir or not os.path.isdir(database_dir):
        raise FileNotFoundError("ALPHAFOLD3_DATABASE_DIR æœªé…ç½®æˆ–ç›®å½•ä¸å­˜åœ¨ï¼Œæ— æ³•è¿è¡Œ AlphaFold3 å®¹å™¨ã€‚")

    visible_devices = os.environ.get("CUDA_VISIBLE_DEVICES")
    gpu_arg = f"device={visible_devices}" if visible_devices else "all"

    container_input_dir = "/workspace/af_input"
    container_output_dir = "/workspace/af_output"
    container_model_dir = "/workspace/models"
    container_database_dir = "/workspace/public_databases"

    docker_command = [
        "docker",
        "run",
        "--rm",
        "--gpus",
        gpu_arg,
        "--volume",
        f"{af3_input_dir}:{container_input_dir}",
        "--volume",
        f"{af3_output_dir}:{container_output_dir}",
        "--volume",
        f"{model_dir}:{container_model_dir}",
        "--volume",
        f"{database_dir}:{container_database_dir}",
    ]

    host_uid = os.getuid()
    host_gid = os.getgid()
    docker_command += [
        "--user",
        f"{host_uid}:{host_gid}",
    ]

    docker_command += extra_args + [
        image,
        "python",
        "run_alphafold.py",
        f"--json_path={container_input_dir}/fold_input.json",
        f"--model_dir={container_model_dir}",
        f"--output_dir={container_output_dir}",
    ]

    display_command = " ".join(shlex.quote(part) for part in docker_command)
    print(f"ğŸ³ è¿è¡Œ AlphaFold3 Docker: {display_command}", file=sys.stderr)
    docker_proc = subprocess.run(
        docker_command,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
    )

    if docker_proc.returncode != 0:
        print(f"âŒ AlphaFold3 Docker è¿è¡Œå¤±è´¥: {docker_proc.stderr}", file=sys.stderr)
        raise RuntimeError(
            f"AlphaFold3 Docker run failed with exit code {docker_proc.returncode}. "
            f"Stdout: {docker_proc.stdout}\nStderr: {docker_proc.stderr}"
        )

    print(f"âœ… AlphaFold3 Docker è¿è¡Œå®Œæˆ: {docker_proc.stdout}", file=sys.stderr)

    af3_output_contents = list(Path(af3_output_dir).rglob("*"))
    if not any(p.is_file() for p in af3_output_contents):
        print("âš ï¸ AlphaFold3 è¾“å‡ºç›®å½•ä¸ºç©ºï¼Œå¯èƒ½æ¨ç†æœªäº§ç”Ÿç»“æœã€‚", file=sys.stderr)

    create_af3_archive(
        output_archive_path,
        fasta_content,
        af3_json,
        chain_msa_paths,
        yaml_content,
        prep,
        af3_output_dir=af3_output_dir,
    )

def main():
    """
    Main function to run a single prediction based on arguments provided in a JSON file.
    The JSON file should contain the necessary parameters for the prediction, including:
    - output_archive_path: Path where the output archive will be saved.
    - yaml_content: YAML content as a string that will be written to a temporary file.
    - Other parameters that will be passed to the predict function as command-line arguments.
    """
    if len(sys.argv) != 2:
        print("Usage: python run_single_prediction.py <args_file_path>")
        sys.exit(1)

    args_file_path = sys.argv[1]

    try:
        with open(args_file_path, 'r') as f:
            predict_args = json.load(f)

        output_archive_path = predict_args.pop("output_archive_path")
        yaml_content = predict_args.pop("yaml_content")
        backend = str(predict_args.pop("backend", "boltz")).strip().lower()
        if backend not in ("boltz", "alphafold3"):
            raise ValueError(f"Unsupported backend '{backend}'.")

        model_name = predict_args.pop("model_name", None)

        use_msa_server = predict_args.get("use_msa_server", False)

        with tempfile.TemporaryDirectory() as temp_dir:
            if backend == "alphafold3":
                run_alphafold3_backend(temp_dir, yaml_content, output_archive_path, use_msa_server)
            else:
                run_boltz_backend(temp_dir, yaml_content, output_archive_path, predict_args, model_name)

            if not os.path.exists(output_archive_path):
                raise FileNotFoundError(
                    f"CRITICAL ERROR: Archive not found at {output_archive_path} immediately after creation."
                )

            print(f"DEBUG: Archive successfully created at: {output_archive_path}", file=sys.stderr)

    except Exception as e:
        print(f"Error during prediction subprocess: {e}\n{traceback.format_exc()}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()
