# run_single_prediction.py
import sys
import os
import json
import tempfile
import shutil
import traceback
import yaml
import hashlib
import glob
import csv
from pathlib import Path

sys.path.append(os.getcwd())
from boltz_wrapper import predict

# MSA ÁºìÂ≠òÈÖçÁΩÆ
MSA_CACHE_CONFIG = {
    'cache_dir': '/tmp/boltz_msa_cache',
    'enable_cache': True
}

def get_sequence_hash(sequence: str) -> str:
    """ËÆ°ÁÆóÂ∫èÂàóÁöÑMD5ÂìàÂ∏åÂÄº‰Ωú‰∏∫ÁºìÂ≠òÈîÆ"""
    return hashlib.md5(sequence.encode('utf-8')).hexdigest()

def cache_msa_files_from_temp_dir(temp_dir: str, yaml_content: str):
    """
    ‰ªé‰∏¥Êó∂ÁõÆÂΩï‰∏≠ÁºìÂ≠òÁîüÊàêÁöÑMSAÊñá‰ª∂
    ÊîØÊåÅ‰ªécolabfold serverÁîüÊàêÁöÑCSVÊ†ºÂºèMSAÊñá‰ª∂
    ‰∏∫ÊØè‰∏™ËõãÁôΩË¥®ÁªÑÂàÜÂçïÁã¨ÁºìÂ≠òMSAÔºåÈÄÇÁî®‰∫éÁªìÊûÑÈ¢ÑÊµãÂíåÂàÜÂ≠êËÆæËÆ°
    """
    if not MSA_CACHE_CONFIG['enable_cache']:
        return
    
    try:
        # Ëß£ÊûêYAMLËé∑ÂèñËõãÁôΩË¥®Â∫èÂàó
        yaml_data = yaml.safe_load(yaml_content)
        protein_sequences = {}
        
        # ÊèêÂèñÊâÄÊúâËõãÁôΩË¥®Â∫èÂàóÔºàÊîØÊåÅÁªìÊûÑÈ¢ÑÊµãÂíåÂàÜÂ≠êËÆæËÆ°Ôºâ
        for entity in yaml_data.get('sequences', []):
            if entity.get('protein', {}).get('id'):
                protein_id = entity['protein']['id']
                sequence = entity['protein'].get('sequence', '')
                if sequence:
                    protein_sequences[protein_id] = sequence
        
        if not protein_sequences:
            print("Êú™ÊâæÂà∞ËõãÁôΩË¥®Â∫èÂàóÔºåË∑≥ËøáMSAÁºìÂ≠ò", file=sys.stderr)
            return
        
        print(f"ÈúÄË¶ÅÁºìÂ≠òÁöÑËõãÁôΩË¥®ÁªÑÂàÜ: {list(protein_sequences.keys())}", file=sys.stderr)
        
        # ËÆæÁΩÆÁºìÂ≠òÁõÆÂΩï
        cache_dir = MSA_CACHE_CONFIG['cache_dir']
        os.makedirs(cache_dir, exist_ok=True)
        
        # ÈÄíÂΩíÊêúÁ¥¢‰∏¥Êó∂ÁõÆÂΩï‰∏≠ÁöÑMSAÊñá‰ª∂
        print(f"ÈÄíÂΩíÊêúÁ¥¢‰∏¥Êó∂ÁõÆÂΩï‰∏≠ÁöÑMSAÊñá‰ª∂: {temp_dir}", file=sys.stderr)
        
        # ‰∏∫ÊØè‰∏™ËõãÁôΩË¥®ÁªÑÂàÜÂçïÁã¨Êü•ÊâæÂØπÂ∫îÁöÑMSAÊñá‰ª∂
        protein_msa_map = {}  # protein_id -> [msa_files]
        
        # ÊêúÁ¥¢ÊâÄÊúâMSAÊñá‰ª∂
        all_msa_files = []
        for root, dirs, files in os.walk(temp_dir):
            for file in files:
                if file.endswith('.csv') or file.endswith('.a3m'):
                    file_path = os.path.join(root, file)
                    all_msa_files.append(file_path)
        
        if not all_msa_files:
            print(f"Âú®‰∏¥Êó∂ÁõÆÂΩï‰∏≠Êú™ÊâæÂà∞‰ªª‰ΩïMSAÊñá‰ª∂: {temp_dir}", file=sys.stderr)
            return
        
        print(f"ÊâæÂà∞ {len(all_msa_files)} ‰∏™MSAÊñá‰ª∂: {[os.path.basename(f) for f in all_msa_files]}", file=sys.stderr)
        
        # ‰∏∫ÊØè‰∏™ËõãÁôΩË¥®ÁªÑÂàÜÂåπÈÖçÂØπÂ∫îÁöÑMSAÊñá‰ª∂
        for protein_id in protein_sequences.keys():
            protein_msa_map[protein_id] = []
            
            for msa_file in all_msa_files:
                filename = os.path.basename(msa_file)
                
                # Á≤æÁ°ÆÂåπÈÖçÔºöÊñá‰ª∂ÂêçÂåÖÂê´protein ID
                if protein_id.lower() in filename.lower():
                    protein_msa_map[protein_id].append(msa_file)
                    continue
                    
                # Á¥¢ÂºïÂåπÈÖçÔºöÂ¶ÇÊûúprotein_idÊòØÂ≠óÊØçÔºåÂ∞ùËØïÂåπÈÖçÂØπÂ∫îÁöÑÊï∞Â≠óÁ¥¢Âºï
                # ‰æãÂ¶ÇÔºöprotein A -> _0.csv, protein B -> _1.csv
                if len(protein_id) == 1 and protein_id.isalpha():
                    protein_index = ord(protein_id.upper()) - ord('A')
                    if f"_{protein_index}." in filename:
                        protein_msa_map[protein_id].append(msa_file)
                        continue
                
                # ÈÄöÁî®ÂåπÈÖçÔºöÂ¶ÇÊûúÂè™Êúâ‰∏Ä‰∏™ËõãÁôΩË¥®ÁªÑÂàÜÔºå‰ΩøÁî®ÈÄöÁî®MSAÊñá‰ª∂
                if len(protein_sequences) == 1 and any(pattern in filename.lower() for pattern in ['msa', '_0.csv', '_0.a3m']):
                    protein_msa_map[protein_id].append(msa_file)
        
        # Â§ÑÁêÜÊØè‰∏™ËõãÁôΩË¥®ÁªÑÂàÜÁöÑMSAÊñá‰ª∂
        cached_count = 0
        for protein_id, msa_files in protein_msa_map.items():
            if not msa_files:
                print(f"‚ùå ËõãÁôΩË¥®ÁªÑÂàÜ {protein_id} Êú™ÊâæÂà∞ÂØπÂ∫îÁöÑMSAÊñá‰ª∂", file=sys.stderr)
                continue
                
            print(f"üîç Â§ÑÁêÜËõãÁôΩË¥®ÁªÑÂàÜ {protein_id} ÁöÑ {len(msa_files)} ‰∏™MSAÊñá‰ª∂", file=sys.stderr)
            
            for msa_file in msa_files:
                if cache_single_protein_msa(protein_id, protein_sequences[protein_id], msa_file, cache_dir):
                    cached_count += 1
                    break  # ÊàêÂäüÁºìÂ≠ò‰∏Ä‰∏™Â∞±Â§ü‰∫Ü
        
        print(f"‚úÖ MSAÁºìÂ≠òÂÆåÊàêÔºåÊàêÂäüÁºìÂ≠ò {cached_count}/{len(protein_sequences)} ‰∏™ËõãÁôΩË¥®ÁªÑÂàÜ", file=sys.stderr)
                
    except Exception as e:
        print(f"‚ùå ÁºìÂ≠òMSAÊñá‰ª∂Â§±Ë¥•: {e}", file=sys.stderr)

def cache_single_protein_msa(protein_id: str, protein_sequence: str, msa_file: str, cache_dir: str) -> bool:
    """
    ‰∏∫Âçï‰∏™ËõãÁôΩË¥®ÁªÑÂàÜÁºìÂ≠òMSAÊñá‰ª∂
    ËøîÂõûÊòØÂê¶ÊàêÂäüÁºìÂ≠ò
    """
    try:
        filename = os.path.basename(msa_file)
        file_ext = os.path.splitext(filename)[1].lower()
        
        print(f"  üìÇ Â§ÑÁêÜMSAÊñá‰ª∂: {filename}", file=sys.stderr)
        
        if file_ext == '.csv':
            # Â§ÑÁêÜCSVÊ†ºÂºèÁöÑMSAÊñá‰ª∂ÔºàÊù•Ëá™colabfold serverÔºâ
            with open(msa_file, 'r') as f:
                reader = csv.reader(f)
                header = next(reader, None)
                if header and len(header) >= 2 and 'sequence' in header:
                    sequences = []
                    for row in reader:
                        if len(row) >= 2 and row[1]:
                            sequences.append(row[1])
                    
                    if sequences:
                        # Á¨¨‰∏Ä‰∏™Â∫èÂàóÈÄöÂ∏∏ÊòØÊü•ËØ¢Â∫èÂàó
                        query_sequence = sequences[0]
                        print(f"    ‰ªéCSVÊèêÂèñÁöÑÊü•ËØ¢Â∫èÂàó: {query_sequence[:50]}...", file=sys.stderr)
                        
                        # È™åËØÅÂ∫èÂàóÊòØÂê¶ÂåπÈÖç
                        if is_sequence_match(protein_sequence, query_sequence):
                            # ËΩ¨Êç¢CSVÊ†ºÂºèÂà∞A3MÊ†ºÂºè
                            a3m_content = f">{protein_id}\n{query_sequence}\n"
                            for i, seq in enumerate(sequences[1:], 1):
                                a3m_content += f">seq_{i}\n{seq}\n"
                            
                            # ÁºìÂ≠òËΩ¨Êç¢ÂêéÁöÑA3MÊñá‰ª∂
                            seq_hash = get_sequence_hash(protein_sequence)
                            cache_path = os.path.join(cache_dir, f"msa_{seq_hash}.a3m")
                            with open(cache_path, 'w') as cache_file:
                                cache_file.write(a3m_content)
                            print(f"    ‚úÖ ÊàêÂäüÁºìÂ≠òËõãÁôΩË¥®ÁªÑÂàÜ {protein_id} ÁöÑMSA (‰ªéCSVËΩ¨Êç¢): {cache_path}", file=sys.stderr)
                            print(f"       Â∫èÂàóÂìàÂ∏å: {seq_hash}", file=sys.stderr)
                            print(f"       MSAÂ∫èÂàóÊï∞: {len(sequences)}", file=sys.stderr)
                            return True
                        else:
                            print(f"    ‚ùå CSVÊñá‰ª∂‰∏≠ÁöÑÊü•ËØ¢Â∫èÂàó‰∏éËõãÁôΩË¥®ÁªÑÂàÜ {protein_id} ‰∏çÂåπÈÖç", file=sys.stderr)
                            return False
        
        elif file_ext == '.a3m':
            # Â§ÑÁêÜA3MÊ†ºÂºèÁöÑMSAÊñá‰ª∂
            with open(msa_file, 'r') as f:
                msa_content = f.read()
            
            # ‰ªéMSAÂÜÖÂÆπ‰∏≠ÊèêÂèñÊü•ËØ¢Â∫èÂàóÔºàÁ¨¨‰∏Ä‰∏™Â∫èÂàóÔºâ
            lines = msa_content.strip().split('\n')
            if len(lines) >= 2 and lines[0].startswith('>'):
                query_sequence = lines[1]
                
                # È™åËØÅÂ∫èÂàóÊòØÂê¶ÂåπÈÖç
                if is_sequence_match(protein_sequence, query_sequence):
                    # ÁºìÂ≠òMSAÊñá‰ª∂
                    seq_hash = get_sequence_hash(protein_sequence)
                    cache_path = os.path.join(cache_dir, f"msa_{seq_hash}.a3m")
                    shutil.copy2(msa_file, cache_path)
                    print(f"    ‚úÖ ÊàêÂäüÁºìÂ≠òËõãÁôΩË¥®ÁªÑÂàÜ {protein_id} ÁöÑMSA: {cache_path}", file=sys.stderr)
                    print(f"       Â∫èÂàóÂìàÂ∏å: {seq_hash}", file=sys.stderr)
                    return True
                else:
                    print(f"    ‚ùå A3MÊñá‰ª∂‰∏≠ÁöÑÊü•ËØ¢Â∫èÂàó‰∏éËõãÁôΩË¥®ÁªÑÂàÜ {protein_id} ‰∏çÂåπÈÖç", file=sys.stderr)
                    return False
        
        return False
        
    except Exception as e:
        print(f"    ‚ùå Â§ÑÁêÜËõãÁôΩË¥®ÁªÑÂàÜ {protein_id} ÁöÑMSAÊñá‰ª∂Â§±Ë¥• {msa_file}: {e}", file=sys.stderr)
        return False

def is_sequence_match(protein_sequence: str, query_sequence: str) -> bool:
    """
    Ê£ÄÊü•ËõãÁôΩË¥®Â∫èÂàóÂíåÊü•ËØ¢Â∫èÂàóÊòØÂê¶ÂåπÈÖç
    ÊîØÊåÅÂÆåÂÖ®ÂåπÈÖç„ÄÅÂÆπÈîôÂåπÈÖçÂíåÁõ∏‰ººÂ∫¶ÂåπÈÖç
    """
    # ÂÆåÂÖ®ÂåπÈÖç
    if protein_sequence == query_sequence:
        return True
    
    # ÂÆπÈîôÂåπÈÖçÔºöÂéªÈô§Á©∫Ê†ºÂíåÁâπÊÆäÂ≠óÁ¨¶ÂêéÊØîËæÉ
    clean_protein = protein_sequence.replace('-', '').replace(' ', '').upper()
    clean_query = query_sequence.replace('-', '').replace(' ', '').upper()
    if clean_protein == clean_query:
        return True
    
    # Â≠êÂ∫èÂàóÂåπÈÖçÔºöÊü•ËØ¢Â∫èÂàóÂèØËÉΩÊòØËõãÁôΩË¥®Â∫èÂàóÁöÑ‰∏ÄÈÉ®ÂàÜ
    if clean_query in clean_protein or clean_protein in clean_query:
        # ËÆ°ÁÆóÁõ∏‰ººÂ∫¶
        similarity = len(set(clean_query) & set(clean_protein)) / max(len(clean_query), len(clean_protein))
        if similarity > 0.8:  # 80%Áõ∏‰ººÂ∫¶ÈòàÂÄº
            return True
    
    return False

def find_results_dir(base_dir: str) -> str:
    result_path = None
    max_depth = -1
    for root, dirs, files in os.walk(base_dir):
        if any(f.endswith((".cif")) for f in files):
            depth = root.count(os.sep)
            if depth > max_depth:
                max_depth = depth
                result_path = root

    if result_path:
        print(f"Found results in directory: {result_path}", file=sys.stderr)
        return result_path

    raise FileNotFoundError(f"Could not find any directory containing result files within the base directory {base_dir}")

def main():
    """
    Main function to run a single prediction based on arguments provided in a JSON file.
    The JSON file should contain the necessary parameters for the prediction, including:
    - output_archive_path: Path where the output archive will be saved.
    - yaml_content: YAML content as a string that will be written to a temporary file.
    - Other parameters that will be passed to the predict function as command-line arguments.
    """
    if len(sys.argv) != 2:
        print("Usage: python run_single_prediction.py <args_file_path>")
        sys.exit(1)

    args_file_path = sys.argv[1]

    try:
        with open(args_file_path, 'r') as f:
            predict_args = json.load(f)

        output_archive_path = predict_args.pop("output_archive_path")

        with tempfile.TemporaryDirectory() as temp_dir:
            yaml_content = predict_args.pop("yaml_content")

            tmp_yaml_path = os.path.join(temp_dir, 'data.yaml')
            with open(tmp_yaml_path, 'w') as tmp_yaml:
                tmp_yaml.write(yaml_content)

            predict_args['data'] = tmp_yaml_path
            predict_args['out_dir'] = temp_dir
            
            POSITIONAL_KEYS = ['data']
            
            cmd_positional = []
            cmd_options = []

            for key, value in predict_args.items():
                if key in POSITIONAL_KEYS:
                    cmd_positional.append(str(value))
                else:
                    if value is None:
                        continue
                    if isinstance(value, bool):
                        if value:
                            cmd_options.append(f'--{key}')
                    else:
                        cmd_options.append(f'--{key}')
                        cmd_options.append(str(value))

            cmd_args = cmd_positional + cmd_options

            print(f"DEBUG: Invoking predict with args: {cmd_args}", file=sys.stderr)
            predict.main(args=cmd_args, standalone_mode=False)

            # Âú®È¢ÑÊµãÂÆåÊàêÂêéÁ´ãÂç≥ÁºìÂ≠òMSAÊñá‰ª∂
            cache_msa_files_from_temp_dir(temp_dir, yaml_content)

            output_directory_path = find_results_dir(temp_dir)

            if not os.listdir(output_directory_path):
                raise NotADirectoryError(f"Prediction result directory was found but is empty: {output_directory_path}")

            archive_base_name = output_archive_path.rsplit('.', 1)[0]
            
            created_archive_path = shutil.make_archive(
                base_name=archive_base_name,
                format='zip',
                root_dir=output_directory_path
            )

            if not os.path.exists(created_archive_path):
                raise FileNotFoundError(f"CRITICAL ERROR: Archive not found at {created_archive_path} immediately after creation.")
            
            print(f"DEBUG: Archive successfully created at: {created_archive_path}", file=sys.stderr)

    except Exception as e:
        print(f"Error during prediction subprocess: {e}\n{traceback.format_exc()}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()