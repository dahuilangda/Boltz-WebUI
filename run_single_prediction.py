# run_single_prediction.py
import sys
import os
import json
import tempfile
import shutil
import traceback
import yaml
import hashlib
import glob
import csv
import zipfile
import shlex
import requests
import time
import tarfile
import io
from pathlib import Path
from typing import Optional, List, Tuple, Dict, Any, Iterable
import subprocess

sys.path.append(os.getcwd())
from boltz_wrapper import predict
from config import (
    MSA_SERVER_URL,
    MSA_SERVER_MODE,
    COLABFOLD_JOBS_DIR,
    ALPHAFOLD3_DOCKER_IMAGE,
    ALPHAFOLD3_MODEL_DIR,
    ALPHAFOLD3_DATABASE_DIR,
    ALPHAFOLD3_DOCKER_EXTRA_ARGS,
)
from af3_adapter import (
    AF3Preparation,
    build_af3_fasta,
    build_af3_json,
    collect_chain_msa_paths,
    load_unpaired_msa,
    parse_yaml_for_af3,
    safe_filename,
    serialize_af3_json,
)

# MSA ç¼“å­˜é…ç½®
MSA_CACHE_CONFIG = {
    'cache_dir': '/tmp/boltz_msa_cache',
    'enable_cache': True
}


def discover_cuda_devices() -> List[str]:
    """Return detected CUDA device indices present on the host."""
    devices: List[str] = []

    try:
        smi_proc = subprocess.run(
            ["nvidia-smi", "-L"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=5,
            check=False,
        )
    except (FileNotFoundError, subprocess.TimeoutExpired):
        smi_proc = None

    if smi_proc and smi_proc.returncode == 0:
        for line in smi_proc.stdout.splitlines():
            line = line.strip()
            if not line.startswith("GPU "):
                continue
            prefix = line.split(':', 1)[0]
            parts = prefix.split()
            if len(parts) >= 2 and parts[1].isdigit():
                devices.append(parts[1])

    if devices:
        return sorted(set(devices), key=int)

    node_paths = Path('/dev').glob('nvidia[0-9]*')
    for node in node_paths:
        suffix = node.name.replace('nvidia', '', 1)
        if suffix.isdigit():
            devices.append(suffix)

    return sorted(set(devices), key=int)


def determine_docker_gpu_arg(visible_devices: Optional[str]) -> str:
    """Validate CUDA availability and build docker --gpus argument."""
    available = discover_cuda_devices()
    if not available:
        raise RuntimeError(
            "AlphaFold3 backend éœ€è¦ NVIDIA GPUï¼Œä½†å½“å‰ç¯å¢ƒæœªæ£€æµ‹åˆ°å¯ç”¨çš„ CUDA è®¾å¤‡ã€‚"
        )

    if not visible_devices:
        return "all"

    tokens = [token.strip() for token in visible_devices.split(',') if token.strip()]
    if not tokens:
        raise RuntimeError("æ£€æµ‹åˆ° CUDA_VISIBLE_DEVICES å·²è®¾ç½®ï¼Œä½†æœªåŒ…å«æœ‰æ•ˆè®¾å¤‡ç´¢å¼•ã€‚")

    numeric_tokens = [token for token in tokens if token.isdigit()]
    invalid = [token for token in numeric_tokens if token not in available]
    if invalid:
        raise RuntimeError(
            "è¯·æ±‚ä½¿ç”¨çš„ GPU ç´¢å¼•åœ¨å½“å‰æœºå™¨ä¸Šä¸å¯ç”¨: "
            f"{', '.join(invalid)}ã€‚å¯ç”¨ç´¢å¼•: {', '.join(available)}"
        )

    return f"device={','.join(tokens)}"


def collect_gpu_device_group_ids() -> List[int]:
    """Capture host group IDs owning GPU device files to re-add inside the container."""
    candidate_nodes = [
        Path("/dev/nvidiactl"),
        Path("/dev/nvidia-uvm"),
        Path("/dev/nvidia-uvm-tools"),
    ]

    candidate_nodes.extend(sorted(Path("/dev").glob("nvidia[0-9]*")))
    candidate_nodes.extend(sorted(Path("/dev/dri").glob("renderD*") if Path("/dev/dri").exists() else []))

    group_ids: List[int] = []
    for node in candidate_nodes:
        try:
            stat_result = node.stat()
        except FileNotFoundError:
            continue
        gid = stat_result.st_gid
        if gid not in group_ids:
            group_ids.append(gid)

    return group_ids


def sanitize_docker_extra_args(raw_args: list) -> list:
    """
    æ¸…ç† Docker é¢å¤–å‚æ•°ï¼Œå¿½ç•¥ä¸å®Œæ•´çš„ --env/-e æ ‡å¿—ä»¥å…åæ‰é•œåƒåç§°ã€‚
    """
    sanitized = []
    i = 0

    while i < len(raw_args):
        token = raw_args[i]

        if token in ("--env", "-e"):
            if i + 1 >= len(raw_args):
                print(f"âš ï¸ å¿½ç•¥æ— æ•ˆçš„ Docker å‚æ•°: {token} (ç¼ºå°‘å€¼)", file=sys.stderr)
                i += 1
                continue

            value = raw_args[i + 1]
            if "=" not in value:
                print(f"âš ï¸ å¿½ç•¥æ— æ•ˆçš„ Docker å‚æ•°: {token} {value} (ç¼ºå°‘ KEY=VALUE å½¢å¼)", file=sys.stderr)
                i += 2
                continue

            sanitized.extend([token, value])
            i += 2
            continue

        sanitized.append(token)
        i += 1

    return sanitized


def sanitize_a3m_content(content: str, context: str = "") -> str:
    """
    ç§»é™¤ A3M å†…å®¹ä¸­çš„éæ³•æ§åˆ¶å­—ç¬¦ï¼ˆä¾‹å¦‚ \\x00ï¼‰ã€‚
    """
    sanitized = content.replace("\x00", "")
    if sanitized != content:
        msg_context = f" ({context})" if context else ""
        print(f"âš ï¸ æ£€æµ‹åˆ°å¹¶ç§»é™¤éæ³•å­—ç¬¦\\x00{msg_context}", file=sys.stderr)
    return sanitized


def sanitize_a3m_file(path: str, context: str = "") -> None:
    """
    å¯¹ A3M æ–‡ä»¶è¿›è¡Œæ¸…ç†ï¼Œç§»é™¤éæ³•æ§åˆ¶å­—ç¬¦ã€‚
    """
    if not os.path.exists(path):
        return

    try:
        with open(path, "r") as f:
            content = f.read()
    except (OSError, UnicodeDecodeError) as e:
        print(f"âš ï¸ æ— æ³•è¯»å– A3M æ–‡ä»¶è¿›è¡Œæ¸…ç†: {path}, {e}", file=sys.stderr)
        return

    sanitized = sanitize_a3m_content(content, context=context or path)
    if sanitized != content:
        try:
            with open(path, "w") as f:
                f.write(sanitized)
        except OSError as e:
            print(f"âš ï¸ æ— æ³•å†™å…¥æ¸…ç†åçš„ A3M æ–‡ä»¶: {path}, {e}", file=sys.stderr)


def _iter_affinity_entries(properties: Any) -> Iterable[Dict[str, Any]]:
    """æ ‡å‡†åŒ– properties å­—æ®µï¼Œæ”¯æŒ list / dict ç­‰å¤šç§å†™æ³•ã€‚"""
    if properties is None:
        return []

    if isinstance(properties, dict):
        # å•ä¸ªå­—å…¸ï¼Œç›´æ¥ä½œä¸ºå€™é€‰
        return [properties]

    if isinstance(properties, list):
        # å·²ç»æ˜¯åˆ—è¡¨ï¼Œè¿‡æ»¤å‡ºå­—å…¸æ¡ç›®
        return [entry for entry in properties if isinstance(entry, dict)]

    # å…¶ä»–ç±»å‹ä¸æ”¯æŒ
    return []


def extract_affinity_config_from_yaml(yaml_data: Dict[str, Any]) -> Optional[Dict[str, str]]:
    """
    ä» YAML æ•°æ®ä¸­æå–äº²å’ŒåŠ›é…ç½®ï¼Œå…¼å®¹ list / dict ç­‰å†™æ³•ã€‚
    æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
    1. affinity: true
    2. affinity: {binder: "B"}
    """
    for entry in _iter_affinity_entries(yaml_data.get("properties")):
        affinity_info = entry.get("affinity")

        # æ ¼å¼1: affinity: {binder: "B"} æˆ– affinity: {chain: "B"}
        if isinstance(affinity_info, dict):
            binder = affinity_info.get("binder") or affinity_info.get("chain")
            if binder:
                return {"binder": str(binder).strip()}

        # æ ¼å¼2: affinity: true (éœ€è¦å•ç‹¬æŸ¥æ‰¾binder)
        elif affinity_info is True:
            # åœ¨åŒä¸€å±‚çº§æˆ–propertieså±‚çº§æŸ¥æ‰¾binderå­—æ®µ
            binder = entry.get("binder") or entry.get("chain")
            if binder:
                return {"binder": str(binder).strip()}

            # å¦‚æœentryä¸­æ²¡æœ‰binderï¼Œå°è¯•ä»propertiesçš„å…¶ä»–æ¡ç›®ä¸­æŸ¥æ‰¾
            for other_entry in _iter_affinity_entries(yaml_data.get("properties")):
                binder = other_entry.get("binder") or other_entry.get("chain")
                if binder:
                    return {"binder": str(binder).strip()}

    return None


def _legacy_parse_ligand_from_text(cif_path: Path, binder_chain: str) -> Optional[str]:
    """åœ¨ç¼ºå°‘ gemmi æ—¶å›é€€åˆ°æ–‡æœ¬è§£æã€‚"""
    try:
        with cif_path.open("r") as cif_file:
            for line in cif_file:
                if not line.startswith("HETATM"):
                    continue
                parts = line.split()
                if len(parts) < 7:
                    continue
                comp_id = parts[5]
                chain_id = parts[6]
                if chain_id == binder_chain:
                    return comp_id
    except OSError as err:
        print(f"âš ï¸ æ— æ³•è¯»å– CIF æ–‡ä»¶ {cif_path}: {err}", file=sys.stderr)
    return None


def find_ligand_resname_in_cif(cif_path: Path, binder_chain: str) -> Optional[str]:
    """
    åœ¨ç»“æ„æ–‡ä»¶ä¸­æŸ¥æ‰¾æŒ‡å®šé“¾çš„é…ä½“æ®‹åŸºåç§°ã€‚
    ä¼˜å…ˆä½¿ç”¨ gemmi è§£æ mmCIF / PDBï¼Œè‹¥ä¸å¯ç”¨åˆ™é€€å›æ–‡æœ¬è§£æã€‚
    """
    try:
        import gemmi  # type: ignore
    except ImportError:
        return _legacy_parse_ligand_from_text(cif_path, binder_chain)

    try:
        structure = gemmi.read_structure(str(cif_path))
    except Exception as err:
        print(f"âš ï¸ æ— æ³•ä½¿ç”¨ gemmi è§£æ {cif_path}: {err}", file=sys.stderr)
        return _legacy_parse_ligand_from_text(cif_path, binder_chain)

    for model in structure:
        chain = next((ch for ch in model if ch.name == binder_chain), None)
        if chain is None:
            continue
        for residue in chain:
            resname = residue.name.strip()
            if resname:
                return resname
    return None


def _sanitize_atom_name_for_affinity(name: str) -> str:
    """Normalize atom names to avoid unsupported characters in Boltz featurizer."""
    cleaned = name.strip()
    if not cleaned:
        return name

    sanitized_chars: List[str] = []
    for ch in cleaned:
        if ch.isalpha():
            sanitized_chars.append(ch.upper())
        elif ch.isdigit():
            sanitized_chars.append(ch)
        else:
            sanitized_chars.append('X')

    sanitized = ''.join(sanitized_chars)
    return sanitized or name


def prepare_structure_for_affinity(source_path: Path, work_dir: Path) -> Path:
    """Create a sanitized copy of the structure with normalized atom names."""
    try:
        import gemmi  # type: ignore
    except ImportError:
        print(
            "âš ï¸ æœªå®‰è£… gemmiï¼Œæ— æ³•æ¸…ç†ç»“æ„åŸå­åï¼Œç›´æ¥ä½¿ç”¨åŸå§‹ç»“æ„ã€‚",
            file=sys.stderr,
        )
        return source_path

    try:
        structure = gemmi.read_structure(str(source_path))
    except Exception as err:
        print(f"âš ï¸ æ— æ³•è¯»å–ç»“æ„ {source_path} è¿›è¡Œæ¸…ç†: {err}", file=sys.stderr)
        return source_path

    changed = False
    for model in structure:
        for chain in model:
            for residue in chain:
                for atom in residue:
                    sanitized = _sanitize_atom_name_for_affinity(atom.name)
                    if sanitized != atom.name:
                        atom.name = sanitized
                        changed = True

    if not changed:
        return source_path

    work_dir.mkdir(parents=True, exist_ok=True)
    sanitized_path = work_dir / f"{source_path.stem}_sanitized{source_path.suffix}"

    try:
        if source_path.suffix.lower() == '.cif':
            doc = structure.make_mmcif_document()
            doc.write_file(str(sanitized_path))
        else:
            structure.write_minimal_pdb(str(sanitized_path))
    except Exception as err:
        print(f"âš ï¸ å†™å…¥æ¸…ç†åçš„ç»“æ„å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹ç»“æ„: {err}", file=sys.stderr)
        return source_path

    print(
        f"ğŸ§¼ å·²ç”Ÿæˆç”¨äºäº²å’ŒåŠ›é¢„æµ‹çš„æ¸…ç†ç»“æ„: {sanitized_path}",
        file=sys.stderr,
    )
    return sanitized_path


def _structure_candidate_priority(name: str, base_priority: int, jobname: str) -> int:
    priority = base_priority
    suffix = Path(name).suffix.lower()
    if suffix == ".cif":
        priority -= 10
    elif suffix == ".pdb":
        priority -= 5

    lowered = name.lower()
    job_lower = jobname.lower()
    if job_lower and job_lower in lowered:
        priority -= 4
    if "ranked_0" in lowered:
        priority -= 2
    if "predicted" in lowered:
        priority -= 1
    if "model" in lowered:
        priority -= 1
    return priority


def locate_af3_structure_file(af3_output_dir: Path, jobname: str) -> Optional[Path]:
    """Locate the primary AlphaFold3 structure file (.cif or .pdb) for affinity post-processing."""
    base_dir = Path(af3_output_dir)
    if not base_dir.exists():
        return None

    candidates: List[Tuple[int, Path]] = []

    def register_candidate(path: Path, base_priority: int) -> None:
        if not path.is_file():
            return
        priority = _structure_candidate_priority(path.name, base_priority, jobname)
        candidates.append((priority, path))

    job_dir = base_dir / jobname
    search_roots: List[Tuple[int, Path]] = []
    if job_dir.exists():
        search_roots.append((0, job_dir))
    search_roots.append((10, base_dir))

    for base_priority, root in search_roots:
        if not root.exists():
            continue
        for path in root.rglob("*.cif"):
            register_candidate(path, base_priority)
        for path in root.rglob("*.pdb"):
            register_candidate(path, base_priority + 2)

    if not candidates:
        return None

    candidates.sort(key=lambda item: (item[0], len(str(item[1]))))
    return candidates[0][1]


def extract_af3_structure_from_archives(
    af3_output_dir: Path,
    scratch_dir: Path,
    jobname: str,
) -> Optional[Path]:
    archive_candidates: List[Tuple[int, Path, str, str]] = []

    job_dir = af3_output_dir / jobname
    archive_patterns = ["*.zip", "*.tar", "*.tar.gz", "*.tgz", "*.tar.xz", "*.tar.bz2"]

    for pattern in archive_patterns:
        for archive_path in af3_output_dir.rglob(pattern):
            base_priority = 60
            try:
                if job_dir.exists() and archive_path.is_relative_to(job_dir):  # type: ignore[attr-defined]
                    base_priority = 40
            except AttributeError:
                try:
                    archive_path.relative_to(job_dir)
                    base_priority = 40
                except ValueError:
                    base_priority = 60

            suffix = archive_path.suffix.lower()
            if archive_path.name.endswith((".tar.gz", ".tgz", ".tar.xz", ".tar.bz2")):
                archive_type = "tar"
            elif suffix in {".tar"}:
                archive_type = "tar"
            else:
                archive_type = "zip"

            if archive_type == "zip":
                try:
                    with zipfile.ZipFile(archive_path) as zf:
                        for info in zf.infolist():
                            if info.is_dir():
                                continue
                            entry_suffix = Path(info.filename).suffix.lower()
                            if entry_suffix not in {".cif", ".pdb"}:
                                continue
                            priority = _structure_candidate_priority(info.filename, base_priority + 10, jobname)
                            archive_candidates.append((priority, archive_path, info.filename, archive_type))
                except (zipfile.BadZipFile, OSError):
                    continue
            else:
                try:
                    with tarfile.open(archive_path, "r:*") as tf:
                        for member in tf.getmembers():
                            if not member.isreg():
                                continue
                            entry_suffix = Path(member.name).suffix.lower()
                            if entry_suffix not in {".cif", ".pdb"}:
                                continue
                            priority = _structure_candidate_priority(member.name, base_priority + 10, jobname)
                            archive_candidates.append((priority, archive_path, member.name, archive_type))
                except (tarfile.TarError, OSError):
                    continue

    if not archive_candidates:
        return None

    archive_candidates.sort(key=lambda item: (item[0], len(item[2])))
    _, selected_archive, selected_member, selected_type = archive_candidates[0]

    scratch_dir.mkdir(parents=True, exist_ok=True)
    member_path = Path(selected_member)
    stem = safe_filename(member_path.stem) or "structure"
    dest_name = stem + member_path.suffix.lower()
    dest_path = scratch_dir / dest_name

    counter = 1
    while dest_path.exists():
        dest_path = scratch_dir / f"{stem}_{counter}{member_path.suffix.lower()}"
        counter += 1

    try:
        if selected_type == "zip":
            with zipfile.ZipFile(selected_archive) as zf:
                with zf.open(selected_member) as source, open(dest_path, "wb") as target:
                    shutil.copyfileobj(source, target)
        else:
            with tarfile.open(selected_archive, "r:*") as tf:
                member = tf.getmember(selected_member)
                extracted = tf.extractfile(member)
                if extracted is None:
                    return None
                with extracted, open(dest_path, "wb") as target:
                    shutil.copyfileobj(extracted, target)
    except (OSError, zipfile.BadZipFile, tarfile.TarError):
        return None

    print(
        f"ğŸ” ä»å½’æ¡£æ–‡ä»¶æå– AlphaFold3 ç»“æ„: {selected_archive} -> {dest_path}",
        file=sys.stderr,
    )
    return dest_path


def run_af3_affinity_pipeline(
    temp_dir: str,
    yaml_data: Dict[str, Any],
    prep: AF3Preparation,
    af3_output_dir: str,
) -> List[Tuple[Path, str]]:
    """
    è‹¥ YAML é…ç½®è¯·æ±‚äº²å’ŒåŠ›é¢„æµ‹ï¼Œåˆ™åœ¨ AlphaFold3 ç»“æœä¸Šè¿è¡Œ Boltz-2 äº²å’ŒåŠ›æµç¨‹ã€‚
    è¿”å›éœ€è¦é™„åŠ åˆ°å½’æ¡£ä¸­çš„é¢å¤–æ–‡ä»¶åˆ—è¡¨ (Path, arcname)ã€‚
    """
    affinity_config = extract_affinity_config_from_yaml(yaml_data)
    if not affinity_config:
        return []

    binder_chain = affinity_config.get("binder")
    if not binder_chain:
        print("â„¹ï¸ äº²å’ŒåŠ›é…ç½®æœªæä¾›æœ‰æ•ˆçš„ binderï¼Œè·³è¿‡äº²å’ŒåŠ›é¢„æµ‹ã€‚", file=sys.stderr)
        return []

    binder_chain = str(binder_chain).strip()
    if not binder_chain:
        print("â„¹ï¸ äº²å’ŒåŠ›é…ç½® binder ä¸ºç©ºï¼Œè·³è¿‡äº²å’ŒåŠ›é¢„æµ‹ã€‚", file=sys.stderr)
        return []

    ligand_entries = [
        entry for entry in yaml_data.get("sequences", [])
        if isinstance(entry, dict) and "ligand" in entry
    ]
    if not ligand_entries:
        print("â„¹ï¸ æœªæ£€æµ‹åˆ°é…ä½“æ¡ç›®ï¼Œè·³è¿‡äº²å’ŒåŠ›é¢„æµ‹ã€‚", file=sys.stderr)
        return []

    binder_chain = prep.chain_id_label_map.get(binder_chain, safe_filename(binder_chain))

    af3_output_path = Path(af3_output_dir)
    model_path = locate_af3_structure_file(af3_output_path, prep.jobname)

    if not model_path or not model_path.exists():
        extracted_path = extract_af3_structure_from_archives(
            af3_output_path,
            Path(temp_dir) / "af3_extracted_structures",
            prep.jobname,
        )
        model_path = extracted_path

    if not model_path or not model_path.exists():
        print(
            "âš ï¸ æœªæ‰¾åˆ° AlphaFold3 é¢„æµ‹çš„ç»“æ„æ–‡ä»¶ï¼Œæ— æ³•è¿›è¡Œäº²å’ŒåŠ›é¢„æµ‹ã€‚",
            file=sys.stderr,
        )
        return []

    print(
        f"ğŸ” ä½¿ç”¨ AlphaFold3 ç»“æ„è¿›è¡Œäº²å’ŒåŠ›è¯„ä¼°: {model_path}",
        file=sys.stderr,
    )

    ligand_resname = find_ligand_resname_in_cif(model_path, binder_chain)
    if not ligand_resname:
        print(
            f"âš ï¸ æœªèƒ½åœ¨ç»“æ„ä¸­æ‰¾åˆ°é“¾ {binder_chain} çš„é…ä½“æ®‹åŸºï¼Œè·³è¿‡äº²å’ŒåŠ›é¢„æµ‹ã€‚",
            file=sys.stderr,
        )
        return []

    try:
        from affinity.main import Boltzina
    except ImportError as err:
        print(f"âš ï¸ æ— æ³•å¯¼å…¥ Boltz-2 äº²å’ŒåŠ›æ¨¡å—ï¼š{err}ï¼Œè·³è¿‡äº²å’ŒåŠ›é¢„æµ‹ã€‚", file=sys.stderr)
        return []

    affinity_base = Path(temp_dir) / "af3_affinity"
    output_dir = affinity_base / "boltzina_output"
    work_dir = affinity_base / "boltzina_work"
    sanitized_struct_dir = affinity_base / "sanitized_structures"

    model_for_affinity = prepare_structure_for_affinity(model_path, sanitized_struct_dir)

    affinity_entries: List[Tuple[Path, str]] = []
    try:
        print(
            f"âš™ï¸ å¼€å§‹è¿è¡Œ Boltz-2 äº²å’ŒåŠ›è¯„ä¼°ï¼Œé…ä½“é“¾: {binder_chain}, æ®‹åŸºå: {ligand_resname}",
            file=sys.stderr,
        )
        boltzina = Boltzina(
            output_dir=str(output_dir),
            work_dir=str(work_dir),
            ligand_resname=ligand_resname,
        )
        boltzina.predict([str(model_for_affinity)])

        if not boltzina.results:
            print("âš ï¸ äº²å’ŒåŠ›é¢„æµ‹æœªäº§ç”Ÿç»“æœï¼Œè·³è¿‡ç”Ÿæˆ affinity_data.jsonã€‚", file=sys.stderr)
            return []

        affinity_result = dict(boltzina.results[0])
        affinity_result["ligand_resname"] = ligand_resname
        affinity_result["binder_chain"] = binder_chain
        affinity_result["source"] = "alphafold3"

        affinity_base.mkdir(parents=True, exist_ok=True)
        affinity_json_path = affinity_base / "affinity_data.json"
        with affinity_json_path.open("w") as json_file:
            json.dump(affinity_result, json_file, indent=2)
        affinity_entries.append((affinity_json_path, "affinity_data.json"))

        affinity_csv_path = output_dir / "affinity_results.csv"
        if affinity_csv_path.exists():
            affinity_entries.append((affinity_csv_path, "af3/affinity_results.csv"))

        print("âœ… äº²å’ŒåŠ›é¢„æµ‹å®Œæˆï¼Œç»“æœå·²å†™å…¥ affinity_data.jsonã€‚", file=sys.stderr)
    except Exception as err:
        print(f"âš ï¸ è¿è¡Œ Boltz-2 äº²å’ŒåŠ›é¢„æµ‹å¤±è´¥: {err}", file=sys.stderr)

    return affinity_entries


def get_sequence_hash(sequence: str) -> str:
    """è®¡ç®—åºåˆ—çš„MD5å“ˆå¸Œå€¼ä½œä¸ºç¼“å­˜é”®"""
    return hashlib.md5(sequence.encode('utf-8')).hexdigest()

def request_msa_from_server(sequence: str, timeout: int = 600) -> dict:
    """
    ä» ColabFold MSA æœåŠ¡å™¨è¯·æ±‚å¤šåºåˆ—æ¯”å¯¹
    
    Args:
        sequence: è›‹ç™½è´¨åºåˆ—ï¼ˆFASTA æ ¼å¼ï¼‰
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    Returns:
        åŒ…å« MSA ç»“æœçš„å­—å…¸ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å› None
    """
    try:
        print(f"ğŸ” æ­£åœ¨ä» MSA æœåŠ¡å™¨è¯·æ±‚å¤šåºåˆ—æ¯”å¯¹: {MSA_SERVER_URL}", file=sys.stderr)
        
        # å‡†å¤‡è¯·æ±‚æ•°æ®
        # ç¡®ä¿åºåˆ—æ˜¯ FASTA æ ¼å¼
        if not sequence.startswith('>'):
            sequence = f">query\n{sequence}"
        
        # ColabFold MSA æœåŠ¡å™¨ä½¿ç”¨ form data æ ¼å¼
        payload = {
            "q": sequence,
            "mode": MSA_SERVER_MODE
        }
        print(f"ğŸ“¦ MSA è¯·æ±‚å‚æ•°: mode={MSA_SERVER_MODE}", file=sys.stderr)
        
        # æäº¤æœç´¢ä»»åŠ¡
        submit_url = f"{MSA_SERVER_URL}/ticket/msa"
        print(f"ğŸ“¤ æäº¤ MSA æœç´¢ä»»åŠ¡åˆ°: {submit_url}", file=sys.stderr)
        
        response = requests.post(submit_url, data=payload, timeout=30)
        if response.status_code != 200:
            print(f"âŒ MSA ä»»åŠ¡æäº¤å¤±è´¥: {response.status_code} - {response.text}", file=sys.stderr)
            return None
        
        result = response.json()
        ticket_id = result.get("id")
        if not ticket_id:
            print(f"âŒ æœªè·å–åˆ°æœ‰æ•ˆçš„ä»»åŠ¡ ID: {result}", file=sys.stderr)
            return None
        
        print(f"âœ… MSA ä»»åŠ¡å·²æäº¤ï¼Œä»»åŠ¡ ID: {ticket_id}", file=sys.stderr)
        
        # è½®è¯¢ç»“æœ
        result_url = f"{MSA_SERVER_URL}/ticket/{ticket_id}"
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            try:
                print(f"â³ æ£€æŸ¥ MSA ä»»åŠ¡çŠ¶æ€...", file=sys.stderr)
                response = requests.get(result_url, timeout=30)
                
                if response.status_code == 200:
                    result_data = response.json()
                    if result_data.get("status") == "COMPLETE":
                        print(f"âœ… MSA æœç´¢å®Œæˆï¼Œè·å–åˆ°ç»“æœ", file=sys.stderr)
                        download_url = result_data.get("result_url") or f"{MSA_SERVER_URL}/result/download/{ticket_id}"
                        print(f"ğŸ“¥ ä¸‹è½½ MSA ç»“æœ: {download_url}", file=sys.stderr)
                        try:
                            download_response = requests.get(download_url, timeout=60)
                        except requests.exceptions.RequestException as download_error:
                            print(f"âŒ ä¸‹è½½ MSA ç»“æœè¯·æ±‚å¤±è´¥: {download_error}", file=sys.stderr)
                            return None
                        if download_response.status_code != 200:
                            print(
                                f"âŒ ä¸‹è½½ MSA ç»“æœå¤±è´¥: {download_response.status_code} - {download_response.text}",
                                file=sys.stderr,
                            )
                            return None

                        try:
                            tar_bytes = io.BytesIO(download_response.content)
                            with tarfile.open(fileobj=tar_bytes, mode="r:gz") as tar:
                                a3m_content = None
                                extracted_filename = None
                                for member in tar.getmembers():
                                    if member.name.lower().endswith(".a3m"):
                                        file_obj = tar.extractfile(member)
                                        if file_obj:
                                            a3m_content = file_obj.read().decode("utf-8")
                                            extracted_filename = member.name
                                            break

                            if not a3m_content:
                                print("âŒ æœªåœ¨ä¸‹è½½çš„ç»“æœä¸­æ‰¾åˆ° A3M æ–‡ä»¶", file=sys.stderr)
                                return None

                            print(f"âœ… æˆåŠŸæå– A3M æ–‡ä»¶: {extracted_filename}", file=sys.stderr)
                            a3m_content = sanitize_a3m_content(a3m_content, context=extracted_filename)
                            entries = parse_a3m_content(a3m_content)
                            return {
                                "entries": entries,
                                "a3m_content": a3m_content,
                                "source": extracted_filename,
                                "ticket_id": ticket_id,
                            }
                        except tarfile.TarError as tar_error:
                            print(f"âŒ è§£æ MSA å‹ç¼©åŒ…å¤±è´¥: {tar_error}", file=sys.stderr)
                            return None
                    elif result_data.get("status") == "ERROR":
                        print(f"âŒ MSA æœç´¢å¤±è´¥: {result_data.get('error', 'æœªçŸ¥é”™è¯¯')}", file=sys.stderr)
                        print(
                            f"   â†³ æœåŠ¡å™¨è¿”å›: {json.dumps(result_data, ensure_ascii=False)}",
                            file=sys.stderr,
                        )
                        return None
                    else:
                        print(f"â³ MSA ä»»åŠ¡çŠ¶æ€: {result_data.get('status', 'PENDING')}", file=sys.stderr)
                elif response.status_code == 404:
                    print(f"â³ ä»»åŠ¡å°šæœªå®Œæˆæˆ–ä¸å­˜åœ¨", file=sys.stderr)
                else:
                    print(f"âš ï¸ æ£€æŸ¥çŠ¶æ€æ—¶å‡ºç°é”™è¯¯: {response.status_code}", file=sys.stderr)
                
            except requests.exceptions.RequestException as e:
                print(f"âš ï¸ æ£€æŸ¥çŠ¶æ€æ—¶ç½‘ç»œé”™è¯¯: {e}", file=sys.stderr)
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´å†æ¬¡æ£€æŸ¥
            time.sleep(10)
        
        print(f"â° MSA æœç´¢è¶…æ—¶ ({timeout}ç§’)", file=sys.stderr)
        return None
        
    except Exception as e:
        print(f"âŒ MSA æœåŠ¡å™¨è¯·æ±‚å¤±è´¥: {e}", file=sys.stderr)
        return None

def save_msa_result_to_file(msa_result: dict, output_path: str) -> bool:
    """
    å°† MSA ç»“æœä¿å­˜åˆ°æ–‡ä»¶
    
    Args:
        msa_result: MSA æœåŠ¡å™¨è¿”å›çš„ç»“æœ
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    
    Returns:
        æ˜¯å¦æˆåŠŸä¿å­˜
    """
    try:
        # æ ¹æ®ç»“æœæ ¼å¼ä¿å­˜ä¸º A3M æ–‡ä»¶
        if msa_result.get('a3m_content'):
            sanitized_content = sanitize_a3m_content(msa_result['a3m_content'], context=output_path)
            with open(output_path, 'w') as f:
                f.write(sanitized_content)
            return True
        elif 'entries' in msa_result:
            buffer = []
            for entry in msa_result['entries']:
                name = entry.get('name', 'unknown')
                sequence = entry.get('sequence', '')
                if sequence:
                    buffer.append(f">{name}\n{sequence}\n")

            sanitized_content = sanitize_a3m_content(''.join(buffer), context=output_path)
            with open(output_path, 'w') as f:
                f.write(sanitized_content)
            return True
        else:
            print(f"âŒ MSA ç»“æœæ ¼å¼ä¸æ”¯æŒ: {msa_result.keys()}", file=sys.stderr)
            return False
            
    except Exception as e:
        print(f"âŒ ä¿å­˜ MSA ç»“æœå¤±è´¥: {e}", file=sys.stderr)
        return False


def parse_a3m_content(a3m_content: str) -> list:
    """
    è§£æ A3M æ–‡ä»¶å†…å®¹ä¸ºåºåˆ—æ¡ç›®åˆ—è¡¨
    """
    sanitized_content = sanitize_a3m_content(a3m_content)
    entries = []
    current_name = None
    current_sequence_lines = []

    for line in sanitized_content.splitlines():
        if line.startswith('>'):
            if current_name is not None:
                entries.append({
                    'name': current_name or 'unknown',
                    'sequence': ''.join(current_sequence_lines),
                })
            current_name = line[1:].strip()
            current_sequence_lines = []
        else:
            current_sequence_lines.append(line.strip())

    if current_name is not None:
        entries.append({
            'name': current_name or 'unknown',
            'sequence': ''.join(current_sequence_lines),
        })

    return entries
def generate_msa_for_sequences(yaml_content: str, temp_dir: str) -> bool:
    """
    ä¸º YAML ä¸­çš„è›‹ç™½è´¨åºåˆ—ç”Ÿæˆ MSA
    
    Args:
        yaml_content: YAML é…ç½®å†…å®¹
        temp_dir: ä¸´æ—¶ç›®å½•
    
    Returns:
        æ˜¯å¦æˆåŠŸç”Ÿæˆ MSA
    """
    try:
        print(f"ğŸ§¬ å¼€å§‹ä¸ºè›‹ç™½è´¨åºåˆ—ç”Ÿæˆ MSA", file=sys.stderr)
        
        # è§£æ YAML è·å–è›‹ç™½è´¨åºåˆ—
        yaml_data = yaml.safe_load(yaml_content)
        protein_sequences = {}
        
        for entity in yaml_data.get('sequences', []):
            if entity.get('protein', {}).get('id'):
                protein_id = entity['protein']['id']
                sequence = entity['protein'].get('sequence', '')
                if sequence:
                    protein_sequences[protein_id] = sequence
        
        if not protein_sequences:
            print("âŒ æœªæ‰¾åˆ°è›‹ç™½è´¨åºåˆ—ï¼Œè·³è¿‡ MSA ç”Ÿæˆ", file=sys.stderr)
            return False
        
        print(f"ğŸ” æ‰¾åˆ° {len(protein_sequences)} ä¸ªè›‹ç™½è´¨åºåˆ—éœ€è¦ç”Ÿæˆ MSA", file=sys.stderr)
        
        # ä¸ºæ¯ä¸ªè›‹ç™½è´¨åºåˆ—ç”Ÿæˆ MSA
        success_count = 0
        for protein_id, sequence in protein_sequences.items():
            print(f"ğŸ§¬ æ­£åœ¨ä¸ºè›‹ç™½è´¨ {protein_id} ç”Ÿæˆ MSA...", file=sys.stderr)
            
            # æ£€æŸ¥ä¸´æ—¶ç›®å½•ä¸­æ˜¯å¦å·²ç»å­˜åœ¨
            output_path = os.path.join(temp_dir, f"{protein_id}_msa.a3m")
            if os.path.exists(output_path):
                print(f"âœ… ä¸´æ—¶ç›®å½•ä¸­å·²å­˜åœ¨ MSA æ–‡ä»¶: {output_path}", file=sys.stderr)
                sanitize_a3m_file(output_path, context=f"{protein_id} ä¸´æ—¶æ–‡ä»¶")
                success_count += 1
                continue
            
            # æ£€æŸ¥ç¼“å­˜ï¼ˆç»Ÿä¸€ä½¿ç”¨ msa_ å‰ç¼€ï¼‰
            sequence_hash = get_sequence_hash(sequence)
            cache_dir = MSA_CACHE_CONFIG['cache_dir']
            cached_msa_path = os.path.join(cache_dir, f"msa_{sequence_hash}.a3m")
            
            if MSA_CACHE_CONFIG['enable_cache'] and os.path.exists(cached_msa_path):
                print(f"âœ… æ‰¾åˆ°ç¼“å­˜çš„ MSA æ–‡ä»¶: {cached_msa_path}", file=sys.stderr)
                sanitize_a3m_file(cached_msa_path, context=f"{protein_id} ç¼“å­˜åŸæ–‡ä»¶")
                # å¤åˆ¶åˆ°ä¸´æ—¶ç›®å½•
                shutil.copy2(cached_msa_path, output_path)
                sanitize_a3m_file(output_path, context=f"{protein_id} ç¼“å­˜å¤åˆ¶")
                success_count += 1
                continue
            
            # ä»æœåŠ¡å™¨è¯·æ±‚ MSA
            msa_result = request_msa_from_server(sequence)
            if msa_result:
                # ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
                if save_msa_result_to_file(msa_result, output_path):
                    sanitize_a3m_file(output_path, context=f"{protein_id} ä¸‹è½½å†™å…¥")
                    success_count += 1
                    
                    # ç¼“å­˜ç»“æœï¼ˆç»Ÿä¸€ä½¿ç”¨ msa_ å‰ç¼€ï¼‰
                    if MSA_CACHE_CONFIG['enable_cache']:
                        os.makedirs(cache_dir, exist_ok=True)
                        shutil.copy2(output_path, cached_msa_path)
                        sanitize_a3m_file(cached_msa_path, context=f"{protein_id} ç¼“å­˜å†™å…¥")
                        print(f"ğŸ’¾ MSA ç»“æœå·²ç¼“å­˜: {cached_msa_path}", file=sys.stderr)
                else:
                    print(f"âŒ ä¿å­˜ MSA æ–‡ä»¶å¤±è´¥: {protein_id}", file=sys.stderr)
            else:
                print(f"âŒ è·å– MSA å¤±è´¥: {protein_id}", file=sys.stderr)
        
        print(f"âœ… MSA ç”Ÿæˆå®Œæˆ: {success_count}/{len(protein_sequences)} ä¸ªæˆåŠŸ", file=sys.stderr)
        return success_count > 0
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆ MSA æ—¶å‡ºç°é”™è¯¯: {e}", file=sys.stderr)
        return False

def cache_msa_files_from_temp_dir(temp_dir: str, yaml_content: str):
    """
    ä»ä¸´æ—¶ç›®å½•ä¸­ç¼“å­˜ç”Ÿæˆçš„MSAæ–‡ä»¶
    æ”¯æŒä»colabfold serverç”Ÿæˆçš„CSVæ ¼å¼MSAæ–‡ä»¶
    ä¸ºæ¯ä¸ªè›‹ç™½è´¨ç»„åˆ†å•ç‹¬ç¼“å­˜MSAï¼Œé€‚ç”¨äºç»“æ„é¢„æµ‹å’Œåˆ†å­è®¾è®¡
    """
    if not MSA_CACHE_CONFIG['enable_cache']:
        return
    
    try:
        # è§£æYAMLè·å–è›‹ç™½è´¨åºåˆ—
        yaml_data = yaml.safe_load(yaml_content)
        protein_sequences = {}
        
        # æå–æ‰€æœ‰è›‹ç™½è´¨åºåˆ—ï¼ˆæ”¯æŒç»“æ„é¢„æµ‹å’Œåˆ†å­è®¾è®¡ï¼‰
        for entity in yaml_data.get('sequences', []):
            if entity.get('protein', {}).get('id'):
                protein_id = entity['protein']['id']
                sequence = entity['protein'].get('sequence', '')
                if sequence:
                    protein_sequences[protein_id] = sequence
        
        if not protein_sequences:
            print("æœªæ‰¾åˆ°è›‹ç™½è´¨åºåˆ—ï¼Œè·³è¿‡MSAç¼“å­˜", file=sys.stderr)
            return
        
        print(f"éœ€è¦ç¼“å­˜çš„è›‹ç™½è´¨ç»„åˆ†: {list(protein_sequences.keys())}", file=sys.stderr)
        
        # è®¾ç½®ç¼“å­˜ç›®å½•
        cache_dir = MSA_CACHE_CONFIG['cache_dir']
        os.makedirs(cache_dir, exist_ok=True)
        
        # é€’å½’æœç´¢ä¸´æ—¶ç›®å½•ä¸­çš„MSAæ–‡ä»¶
        print(f"é€’å½’æœç´¢ä¸´æ—¶ç›®å½•ä¸­çš„MSAæ–‡ä»¶: {temp_dir}", file=sys.stderr)
        
        # ä¸ºæ¯ä¸ªè›‹ç™½è´¨ç»„åˆ†å•ç‹¬æŸ¥æ‰¾å¯¹åº”çš„MSAæ–‡ä»¶
        protein_msa_map = {}  # protein_id -> [msa_files]
        
        # æœç´¢æ‰€æœ‰MSAæ–‡ä»¶
        all_msa_files = []
        for root, dirs, files in os.walk(temp_dir):
            for file in files:
                if file.endswith('.csv') or file.endswith('.a3m'):
                    file_path = os.path.join(root, file)
                    all_msa_files.append(file_path)
        
        if not all_msa_files:
            print(f"åœ¨ä¸´æ—¶ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•MSAæ–‡ä»¶: {temp_dir}", file=sys.stderr)
            return
        
        print(f"æ‰¾åˆ° {len(all_msa_files)} ä¸ªMSAæ–‡ä»¶: {[os.path.basename(f) for f in all_msa_files]}", file=sys.stderr)
        
        # ä¸ºæ¯ä¸ªè›‹ç™½è´¨ç»„åˆ†åŒ¹é…å¯¹åº”çš„MSAæ–‡ä»¶
        for protein_id in protein_sequences.keys():
            protein_msa_map[protein_id] = []
            
            for msa_file in all_msa_files:
                filename = os.path.basename(msa_file)
                
                # ç²¾ç¡®åŒ¹é…ï¼šæ–‡ä»¶ååŒ…å«protein ID
                if protein_id.lower() in filename.lower():
                    protein_msa_map[protein_id].append(msa_file)
                    continue
                    
                # ç´¢å¼•åŒ¹é…ï¼šå¦‚æœprotein_idæ˜¯å­—æ¯ï¼Œå°è¯•åŒ¹é…å¯¹åº”çš„æ•°å­—ç´¢å¼•
                # ä¾‹å¦‚ï¼šprotein A -> _0.csv, protein B -> _1.csv
                if len(protein_id) == 1 and protein_id.isalpha():
                    protein_index = ord(protein_id.upper()) - ord('A')
                    if f"_{protein_index}." in filename:
                        protein_msa_map[protein_id].append(msa_file)
                        continue
                
                # é€šç”¨åŒ¹é…ï¼šå¦‚æœåªæœ‰ä¸€ä¸ªè›‹ç™½è´¨ç»„åˆ†ï¼Œä½¿ç”¨é€šç”¨MSAæ–‡ä»¶
                if len(protein_sequences) == 1 and any(pattern in filename.lower() for pattern in ['msa', '_0.csv', '_0.a3m']):
                    protein_msa_map[protein_id].append(msa_file)
        
        # å¤„ç†æ¯ä¸ªè›‹ç™½è´¨ç»„åˆ†çš„MSAæ–‡ä»¶
        cached_count = 0
        for protein_id, msa_files in protein_msa_map.items():
            if not msa_files:
                print(f"âŒ è›‹ç™½è´¨ç»„åˆ† {protein_id} æœªæ‰¾åˆ°å¯¹åº”çš„MSAæ–‡ä»¶", file=sys.stderr)
                continue
                
            print(f"ğŸ” å¤„ç†è›‹ç™½è´¨ç»„åˆ† {protein_id} çš„ {len(msa_files)} ä¸ªMSAæ–‡ä»¶", file=sys.stderr)
            
            for msa_file in msa_files:
                if cache_single_protein_msa(protein_id, protein_sequences[protein_id], msa_file, cache_dir):
                    cached_count += 1
                    break  # æˆåŠŸç¼“å­˜ä¸€ä¸ªå°±å¤Ÿäº†
        
        print(f"âœ… MSAç¼“å­˜å®Œæˆï¼ŒæˆåŠŸç¼“å­˜ {cached_count}/{len(protein_sequences)} ä¸ªè›‹ç™½è´¨ç»„åˆ†", file=sys.stderr)
                
    except Exception as e:
        print(f"âŒ ç¼“å­˜MSAæ–‡ä»¶å¤±è´¥: {e}", file=sys.stderr)

def cache_single_protein_msa(protein_id: str, protein_sequence: str, msa_file: str, cache_dir: str) -> bool:
    """
    ä¸ºå•ä¸ªè›‹ç™½è´¨ç»„åˆ†ç¼“å­˜MSAæ–‡ä»¶
    è¿”å›æ˜¯å¦æˆåŠŸç¼“å­˜
    """
    try:
        filename = os.path.basename(msa_file)
        file_ext = os.path.splitext(filename)[1].lower()
        
        print(f"  ğŸ“‚ å¤„ç†MSAæ–‡ä»¶: {filename}", file=sys.stderr)
        
        if file_ext == '.csv':
            # å¤„ç†CSVæ ¼å¼çš„MSAæ–‡ä»¶ï¼ˆæ¥è‡ªcolabfold serverï¼‰
            with open(msa_file, 'r') as f:
                reader = csv.reader(f)
                header = next(reader, None)
                if header and len(header) >= 2 and 'sequence' in header:
                    sequences = []
                    for row in reader:
                        if len(row) >= 2 and row[1]:
                            sequences.append(row[1])
                    
                    if sequences:
                        # ç¬¬ä¸€ä¸ªåºåˆ—é€šå¸¸æ˜¯æŸ¥è¯¢åºåˆ—
                        query_sequence = sequences[0]
                        print(f"    ä»CSVæå–çš„æŸ¥è¯¢åºåˆ—: {query_sequence[:50]}...", file=sys.stderr)
                        
                        # éªŒè¯åºåˆ—æ˜¯å¦åŒ¹é…
                        if is_sequence_match(protein_sequence, query_sequence):
                            # è½¬æ¢CSVæ ¼å¼åˆ°A3Mæ ¼å¼
                            a3m_content = f">{protein_id}\n{query_sequence}\n"
                            for i, seq in enumerate(sequences[1:], 1):
                                a3m_content += f">seq_{i}\n{seq}\n"
                            
                            # ç¼“å­˜è½¬æ¢åçš„A3Mæ–‡ä»¶
                            seq_hash = get_sequence_hash(protein_sequence)
                            cache_path = os.path.join(cache_dir, f"msa_{seq_hash}.a3m")
                            with open(cache_path, 'w') as cache_file:
                                cache_file.write(sanitize_a3m_content(a3m_content, context=f"{protein_id} CSV è½¬æ¢"))
                            print(f"    âœ… æˆåŠŸç¼“å­˜è›‹ç™½è´¨ç»„åˆ† {protein_id} çš„MSA (ä»CSVè½¬æ¢): {cache_path}", file=sys.stderr)
                            print(f"       åºåˆ—å“ˆå¸Œ: {seq_hash}", file=sys.stderr)
                            print(f"       MSAåºåˆ—æ•°: {len(sequences)}", file=sys.stderr)
                            return True
                        else:
                            print(f"    âŒ CSVæ–‡ä»¶ä¸­çš„æŸ¥è¯¢åºåˆ—ä¸è›‹ç™½è´¨ç»„åˆ† {protein_id} ä¸åŒ¹é…", file=sys.stderr)
                            return False
        
        elif file_ext == '.a3m':
            # å¤„ç†A3Mæ ¼å¼çš„MSAæ–‡ä»¶
            sanitize_a3m_file(msa_file, context=f"{protein_id} æºMSA")
            with open(msa_file, 'r') as f:
                msa_content = sanitize_a3m_content(f.read(), context=msa_file)
            
            # ä»MSAå†…å®¹ä¸­æå–æŸ¥è¯¢åºåˆ—ï¼ˆç¬¬ä¸€ä¸ªåºåˆ—ï¼‰
            lines = msa_content.strip().split('\n')
            if len(lines) >= 2 and lines[0].startswith('>'):
                query_sequence = lines[1]
                
                # éªŒè¯åºåˆ—æ˜¯å¦åŒ¹é…
                if is_sequence_match(protein_sequence, query_sequence):
                    # ç¼“å­˜MSAæ–‡ä»¶
                    seq_hash = get_sequence_hash(protein_sequence)
                    cache_path = os.path.join(cache_dir, f"msa_{seq_hash}.a3m")
                    with open(cache_path, 'w') as cache_file:
                        cache_file.write(msa_content)
                    print(f"    âœ… æˆåŠŸç¼“å­˜è›‹ç™½è´¨ç»„åˆ† {protein_id} çš„MSA: {cache_path}", file=sys.stderr)
                    print(f"       åºåˆ—å“ˆå¸Œ: {seq_hash}", file=sys.stderr)
                    return True
                else:
                    print(f"    âŒ A3Mæ–‡ä»¶ä¸­çš„æŸ¥è¯¢åºåˆ—ä¸è›‹ç™½è´¨ç»„åˆ† {protein_id} ä¸åŒ¹é…", file=sys.stderr)
                    return False
        
        return False
        
    except Exception as e:
        print(f"    âŒ å¤„ç†è›‹ç™½è´¨ç»„åˆ† {protein_id} çš„MSAæ–‡ä»¶å¤±è´¥ {msa_file}: {e}", file=sys.stderr)
        return False

def is_sequence_match(protein_sequence: str, query_sequence: str) -> bool:
    """
    æ£€æŸ¥è›‹ç™½è´¨åºåˆ—å’ŒæŸ¥è¯¢åºåˆ—æ˜¯å¦åŒ¹é…
    æ”¯æŒå®Œå…¨åŒ¹é…ã€å®¹é”™åŒ¹é…å’Œç›¸ä¼¼åº¦åŒ¹é…
    """
    # å®Œå…¨åŒ¹é…
    if protein_sequence == query_sequence:
        return True
    
    # å®¹é”™åŒ¹é…ï¼šå»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦åæ¯”è¾ƒ
    clean_protein = protein_sequence.replace('-', '').replace(' ', '').upper()
    clean_query = query_sequence.replace('-', '').replace(' ', '').upper()
    if clean_protein == clean_query:
        return True
    
    # å­åºåˆ—åŒ¹é…ï¼šæŸ¥è¯¢åºåˆ—å¯èƒ½æ˜¯è›‹ç™½è´¨åºåˆ—çš„ä¸€éƒ¨åˆ†
    if clean_query in clean_protein or clean_protein in clean_query:
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = len(set(clean_query) & set(clean_protein)) / max(len(clean_query), len(clean_protein))
        if similarity > 0.8:  # 80%ç›¸ä¼¼åº¦é˜ˆå€¼
            return True
    
    return False

def find_results_dir(base_dir: str) -> str:
    result_path = None
    max_depth = -1
    for root, dirs, files in os.walk(base_dir):
        if any(f.endswith((".cif")) for f in files):
            depth = root.count(os.sep)
            if depth > max_depth:
                max_depth = depth
                result_path = root

    if result_path:
        print(f"Found results in directory: {result_path}", file=sys.stderr)
        return result_path

    raise FileNotFoundError(f"Could not find any directory containing result files within the base directory {base_dir}")

def get_cached_a3m_files(yaml_content: str) -> list:
    """
    è·å–ä¸å½“å‰é¢„æµ‹ä»»åŠ¡ç›¸å…³çš„a3mç¼“å­˜æ–‡ä»¶
    è¿”å›ç¼“å­˜æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    cached_a3m_files = []
    
    if not MSA_CACHE_CONFIG['enable_cache']:
        return cached_a3m_files
    
    try:
        # è§£æYAMLè·å–è›‹ç™½è´¨åºåˆ—
        yaml_data = yaml.safe_load(yaml_content)
        protein_sequences = {}
        
        # æå–æ‰€æœ‰è›‹ç™½è´¨åºåˆ—
        for entity in yaml_data.get('sequences', []):
            if entity.get('protein', {}).get('id'):
                protein_id = entity['protein']['id']
                sequence = entity['protein'].get('sequence', '')
                if sequence:
                    protein_sequences[protein_id] = sequence
        
        if not protein_sequences:
            print("æœªæ‰¾åˆ°è›‹ç™½è´¨åºåˆ—ï¼Œè·³è¿‡a3mæ–‡ä»¶æ”¶é›†", file=sys.stderr)
            return cached_a3m_files
        
        cache_dir = MSA_CACHE_CONFIG['cache_dir']
        if not os.path.exists(cache_dir):
            return cached_a3m_files
        
        print(f"æŸ¥æ‰¾ç¼“å­˜çš„a3mæ–‡ä»¶ï¼Œè›‹ç™½è´¨ç»„åˆ†: {list(protein_sequences.keys())}", file=sys.stderr)
        
        # ä¸ºæ¯ä¸ªè›‹ç™½è´¨åºåˆ—æŸ¥æ‰¾å¯¹åº”çš„ç¼“å­˜æ–‡ä»¶
        for protein_id, sequence in protein_sequences.items():
            seq_hash = get_sequence_hash(sequence)
            cache_file_path = os.path.join(cache_dir, f"msa_{seq_hash}.a3m")
            
            if os.path.exists(cache_file_path):
                cached_a3m_files.append({
                    'path': cache_file_path,
                    'protein_id': protein_id,
                    'filename': f"{protein_id}_msa.a3m"
                })
                print(f"æ‰¾åˆ°ç¼“å­˜æ–‡ä»¶: {protein_id} -> {cache_file_path}", file=sys.stderr)
        
        print(f"æ€»å…±æ‰¾åˆ° {len(cached_a3m_files)} ä¸ªa3mç¼“å­˜æ–‡ä»¶", file=sys.stderr)
        
    except Exception as e:
        print(f"è·å–a3mç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}", file=sys.stderr)
    
    return cached_a3m_files

def create_archive_with_a3m(output_archive_path: str, output_directory_path: str, yaml_content: str):
    """
    åˆ›å»ºåŒ…å«é¢„æµ‹ç»“æœå’Œa3mç¼“å­˜æ–‡ä»¶çš„zipå½’æ¡£
    """
    try:
        # è·å–ç›¸å…³çš„a3mç¼“å­˜æ–‡ä»¶
        cached_a3m_files = get_cached_a3m_files(yaml_content)
        
        # åˆ›å»ºzipæ–‡ä»¶
        with zipfile.ZipFile(output_archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            # æ·»åŠ é¢„æµ‹ç»“æœæ–‡ä»¶
            for root, dirs, files in os.walk(output_directory_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œä¿æŒç›®å½•ç»“æ„
                    arcname = os.path.relpath(file_path, output_directory_path)
                    zipf.write(file_path, arcname)
                    print(f"æ·»åŠ ç»“æœæ–‡ä»¶: {arcname}", file=sys.stderr)
            
            # æ·»åŠ a3mç¼“å­˜æ–‡ä»¶
            if cached_a3m_files:
                # åœ¨zipä¸­åˆ›å»ºmsaç›®å½•
                for a3m_info in cached_a3m_files:
                    cache_file_path = a3m_info['path']
                    filename = a3m_info['filename']
                    # å°†a3mæ–‡ä»¶æ”¾åœ¨msaå­ç›®å½•ä¸­
                    arcname = f"msa/{filename}"
                    zipf.write(cache_file_path, arcname)
                    print(f"æ·»åŠ a3mç¼“å­˜æ–‡ä»¶: {arcname}", file=sys.stderr)
                
                print(f"âœ… æˆåŠŸæ·»åŠ  {len(cached_a3m_files)} ä¸ªa3mç¼“å­˜æ–‡ä»¶åˆ°zipå½’æ¡£", file=sys.stderr)
            else:
                print("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³çš„a3mç¼“å­˜æ–‡ä»¶", file=sys.stderr)
        
        print(f"âœ… å½’æ¡£åˆ›å»ºå®Œæˆ: {output_archive_path}", file=sys.stderr)
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºåŒ…å«a3mæ–‡ä»¶çš„å½’æ¡£å¤±è´¥: {e}", file=sys.stderr)
        # å¦‚æœå¤±è´¥ï¼Œå›é€€åˆ°åŸæ¥çš„æ–¹å¼
        archive_base_name = output_archive_path.rsplit('.', 1)[0]
        created_archive_path = shutil.make_archive(
            base_name=archive_base_name,
            format='zip',
            root_dir=output_directory_path
        )
        print(f"å›é€€åˆ°æ ‡å‡†å½’æ¡£æ–¹å¼: {created_archive_path}", file=sys.stderr)


def create_af3_archive(
    output_archive_path: str,
    fasta_content: str,
    af3_json: dict,
    chain_msa_paths: dict,
    yaml_content: str,
    prep: AF3Preparation,
    af3_output_dir: Optional[str] = None,
    extra_files: Optional[List[Tuple[Path, str]]] = None,
) -> None:
    """
    Create an archive containing AF3-compatible assets (FASTA, JSON, and MSAs).
    """
    try:
        with zipfile.ZipFile(output_archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            zipf.writestr(f"af3/{prep.jobname}_input.fasta", fasta_content)
            zipf.writestr(f"af3/{prep.jobname}_input.json", serialize_af3_json(af3_json))
            zipf.writestr("af3/input.yaml", yaml_content)

            metadata = {
                "jobname": prep.jobname,
                "chain_labels": prep.header_labels,
                "sequence_cardinality": prep.query_sequences_cardinality,
                "chain_id_label_map": prep.chain_id_label_map,
            }
            zipf.writestr("af3/metadata.json", json.dumps(metadata, indent=2, ensure_ascii=False))

            if chain_msa_paths:
                for chain_id, path in chain_msa_paths.items():
                    if not path or not os.path.exists(path):
                        continue
                    arcname = f"af3/msa/{safe_filename(chain_id)}.a3m"
                    zipf.write(path, arcname)
                    print(f"æ·»åŠ AF3 MSAæ–‡ä»¶: {arcname}", file=sys.stderr)
            else:
                print("âš ï¸ æœªæ‰¾åˆ°AF3æ‰€éœ€çš„MSAæ–‡ä»¶ï¼ŒJSONä¸­å°†ç•™ç©º", file=sys.stderr)

            output_files_added = False
            if af3_output_dir and os.path.isdir(af3_output_dir):
                for root, _, files in os.walk(af3_output_dir):
                    for file in files:
                        file_path = os.path.join(root, file)
                        arcname = os.path.relpath(file_path, af3_output_dir)
                        arcname = os.path.join("af3/output", arcname)
                        zipf.write(file_path, arcname)
                        print(f"æ·»åŠ AF3è¾“å‡ºæ–‡ä»¶: {arcname}", file=sys.stderr)
                        output_files_added = True
            if not output_files_added:
                print("â„¹ï¸ AF3è¾“å‡ºç›®å½•ä¸ºç©ºæˆ–ç¼ºå¤±ï¼Œä»…ä¿ç•™è¾“å…¥æ–‡ä»¶", file=sys.stderr)

            instructions = (
                "AlphaFold3 input assets generated by Boltz-WebUI.\n"
                "Files included:\n"
                " - af3_input.fasta / af3_input.json: ready for AlphaFold3 jobs\n"
                " - msa directory: cached MSAs per chain (if available)\n"
                " - input.yaml: original request payload\n"
                " - output/: files produced by AlphaFold3 (if the docker run succeeded)\n"
                "\n"
                "Upload the JSON file to AlphaFold3 alongside the FASTA sequence.\n"
            )
            zipf.writestr("af3/README.txt", instructions)

            if extra_files:
                for file_path, arcname in extra_files:
                    if not file_path or not Path(file_path).exists():
                        print(f"âš ï¸ é¢å¤–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡æ·»åŠ : {file_path}", file=sys.stderr)
                        continue
                    zipf.write(str(file_path), arcname)
                    print(f"æ·»åŠ é¢å¤–æ–‡ä»¶: {arcname}", file=sys.stderr)

        print(f"âœ… AF3 å½’æ¡£åˆ›å»ºå®Œæˆ: {output_archive_path}", file=sys.stderr)
    except Exception as e:
        raise RuntimeError(f"Failed to create AF3 archive: {e}") from e


def run_boltz_backend(
    temp_dir: str,
    yaml_content: str,
    output_archive_path: str,
    predict_args: dict,
    model_name: Optional[str],
) -> None:
    tmp_yaml_path = os.path.join(temp_dir, 'data.yaml')
    with open(tmp_yaml_path, 'w') as tmp_yaml:
        tmp_yaml.write(yaml_content)

    cli_args = dict(predict_args)
    if model_name:
        cli_args['model'] = model_name
        print(f"DEBUG: Using model: {model_name}", file=sys.stderr)

    cli_args['data'] = tmp_yaml_path
    cli_args['out_dir'] = temp_dir

    if MSA_SERVER_URL and MSA_SERVER_URL != "":
        print(f"ğŸ§¬ å¼€å§‹ä½¿ç”¨ MSA æœåŠ¡å™¨ç”Ÿæˆå¤šåºåˆ—æ¯”å¯¹: {MSA_SERVER_URL}", file=sys.stderr)
        msa_generated = generate_msa_for_sequences(yaml_content, temp_dir)
        if msa_generated:
            print(f"âœ… MSA ç”ŸæˆæˆåŠŸï¼Œå°†ç”¨äºç»“æ„é¢„æµ‹", file=sys.stderr)
        else:
            print(f"âš ï¸ MSA ç”Ÿæˆå¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤æ–¹æ³•è¿›è¡Œé¢„æµ‹", file=sys.stderr)
    else:
        print(f"â„¹ï¸ æœªé…ç½® MSA æœåŠ¡å™¨ï¼Œè·³è¿‡ MSA ç”Ÿæˆ", file=sys.stderr)

    POSITIONAL_KEYS = ['data']
    cmd_positional = []
    cmd_options = []

    for key, value in cli_args.items():
        if key in POSITIONAL_KEYS:
            cmd_positional.append(str(value))
        else:
            if value is None:
                continue
            if isinstance(value, bool):
                if value:
                    cmd_options.append(f'--{key}')
            else:
                cmd_options.append(f'--{key}')
                cmd_options.append(str(value))

    cmd_args = cmd_positional + cmd_options

    print(f"DEBUG: Invoking predict with args: {cmd_args}", file=sys.stderr)
    predict.main(args=cmd_args, standalone_mode=False)

    cache_msa_files_from_temp_dir(temp_dir, yaml_content)

    output_directory_path = find_results_dir(temp_dir)
    if not os.listdir(output_directory_path):
        raise NotADirectoryError(
            f"Prediction result directory was found but is empty: {output_directory_path}"
        )

    create_archive_with_a3m(output_archive_path, output_directory_path, yaml_content)


def run_alphafold3_backend(
    temp_dir: str,
    yaml_content: str,
    output_archive_path: str,
    use_msa_server: bool,
) -> None:
    print("ğŸš€ Using AlphaFold3 backend (AF3 input preparation)", file=sys.stderr)

    try:
        yaml_data = yaml.safe_load(yaml_content) or {}
    except yaml.YAMLError as err:
        print(f"âš ï¸ æ— æ³•è§£æ YAMLï¼Œäº²å’ŒåŠ›åå¤„ç†å°†è¢«è·³è¿‡: {err}", file=sys.stderr)
        yaml_data = {}

    if use_msa_server and MSA_SERVER_URL and MSA_SERVER_URL != "":
        print(f"ğŸ§¬ å¼€å§‹ä½¿ç”¨ MSA æœåŠ¡å™¨ç”Ÿæˆå¤šåºåˆ—æ¯”å¯¹: {MSA_SERVER_URL}", file=sys.stderr)
        msa_generated = generate_msa_for_sequences(yaml_content, temp_dir)
        if msa_generated:
            print(f"âœ… MSA ç”ŸæˆæˆåŠŸï¼Œå°†ç”¨äºAF3è¾“å…¥", file=sys.stderr)
        else:
            print(f"âš ï¸ æœªèƒ½è·å–MSAï¼ŒAF3 JSONå°†å«ç©ºMSAå­—æ®µ", file=sys.stderr)
    else:
        print("â„¹ï¸ æœªé…ç½® MSA æœåŠ¡å™¨æˆ–æœªè¯·æ±‚ä½¿ç”¨ï¼Œå°†å°è¯•ä½¿ç”¨ç¼“å­˜çš„MSA", file=sys.stderr)

    prep = parse_yaml_for_af3(yaml_content)
    cache_dir = MSA_CACHE_CONFIG['cache_dir'] if MSA_CACHE_CONFIG['enable_cache'] else None
    chain_msa_paths = collect_chain_msa_paths(prep, temp_dir, cache_dir)
    unpaired_msa = load_unpaired_msa(prep, chain_msa_paths)
    fasta_content = build_af3_fasta(prep)
    af3_json = build_af3_json(prep, unpaired_msa)

    cache_msa_files_from_temp_dir(temp_dir, yaml_content)

    af3_input_dir = os.path.join(temp_dir, "af3_input")
    af3_output_dir = os.path.join(temp_dir, "af3_output")
    os.makedirs(af3_input_dir, exist_ok=True)
    os.makedirs(af3_output_dir, exist_ok=True)

    fasta_path = os.path.join(af3_input_dir, f"{prep.jobname}_input.fasta")
    json_path = os.path.join(af3_input_dir, "fold_input.json")

    with open(fasta_path, "w") as fasta_file:
        fasta_file.write(fasta_content)
    with open(json_path, "w") as json_file:
        json.dump(af3_json, json_file, indent=2, ensure_ascii=False)

    model_dir = ALPHAFOLD3_MODEL_DIR
    database_dir = ALPHAFOLD3_DATABASE_DIR
    image = ALPHAFOLD3_DOCKER_IMAGE or "alphafold3"
    raw_extra_args = shlex.split(ALPHAFOLD3_DOCKER_EXTRA_ARGS) if ALPHAFOLD3_DOCKER_EXTRA_ARGS else []
    extra_args = sanitize_docker_extra_args(raw_extra_args)
    if raw_extra_args and len(extra_args) != len(raw_extra_args):
        print(
            f"âš ï¸ å·²å¿½ç•¥éƒ¨åˆ† ALPHAFOLD3_DOCKER_EXTRA_ARGS å‚æ•°ï¼ŒåŸå§‹å€¼: {raw_extra_args}",
            file=sys.stderr,
        )

    if not model_dir or not os.path.isdir(model_dir):
        raise FileNotFoundError("ALPHAFOLD3_MODEL_DIR æœªé…ç½®æˆ–ç›®å½•ä¸å­˜åœ¨ï¼Œæ— æ³•è¿è¡Œ AlphaFold3 å®¹å™¨ã€‚")
    if not database_dir or not os.path.isdir(database_dir):
        raise FileNotFoundError("ALPHAFOLD3_DATABASE_DIR æœªé…ç½®æˆ–ç›®å½•ä¸å­˜åœ¨ï¼Œæ— æ³•è¿è¡Œ AlphaFold3 å®¹å™¨ã€‚")

    visible_devices = os.environ.get("CUDA_VISIBLE_DEVICES")
    try:
        gpu_arg = determine_docker_gpu_arg(visible_devices)
    except RuntimeError as gpu_err:
        print(f"âŒ æ— æ³•å‡†å¤‡ AlphaFold3 GPU ç¯å¢ƒ: {gpu_err}", file=sys.stderr)
        print("   â†³ è¯·ç¡®è®¤æ­¤ä¸»æœºå®‰è£…äº† NVIDIA é©±åŠ¨å¹¶æ­£ç¡®è®¾ç½® CUDA_VISIBLE_DEVICESã€‚", file=sys.stderr)
        raise

    container_input_dir = "/workspace/af_input"
    container_output_dir = "/workspace/af_output"
    container_model_dir = "/workspace/models"
    container_database_dir = "/workspace/public_databases"
    container_colabfold_jobs_dir = "/app/jobs"

    runtime_overridden = any(token == "--runtime" for token in extra_args)

    docker_command = [
        "docker",
        "run",
        "--rm",
    ]

    if not runtime_overridden:
        docker_command.extend(["--runtime", "nvidia"])

    docker_command.extend(
        [
            "--gpus",
            gpu_arg,
            "--volume",
            f"{af3_input_dir}:{container_input_dir}",
            "--volume",
            f"{af3_output_dir}:{container_output_dir}",
            "--volume",
            f"{model_dir}:{container_model_dir}",
            "--volume",
            f"{database_dir}:{container_database_dir}",
        ]
    )

    # æ·»åŠ  ColabFold jobs ç›®å½•æŒ‚è½½ï¼ˆå¦‚æœé…ç½®äº† MSA æœåŠ¡å™¨ï¼‰
    if MSA_SERVER_URL and COLABFOLD_JOBS_DIR and os.path.exists(COLABFOLD_JOBS_DIR):
        docker_command.extend([
            "--volume",
            f"{COLABFOLD_JOBS_DIR}:{container_colabfold_jobs_dir}",
        ])
        print(f"ğŸ”— æŒ‚è½½ ColabFold jobs ç›®å½•: {COLABFOLD_JOBS_DIR} -> {container_colabfold_jobs_dir}", file=sys.stderr)
    else:
        print("âš ï¸ æœªæ‰¾åˆ° ColabFold jobs ç›®å½•æˆ–æœªé…ç½® MSA æœåŠ¡å™¨", file=sys.stderr)

    host_uid = os.getuid()
    host_gid = os.getgid()
    docker_command += [
        "--user",
        f"{host_uid}:{host_gid}",
    ]

    gpu_device_groups = collect_gpu_device_group_ids()
    if not gpu_device_groups:
        print("âš ï¸ æœªèƒ½æ£€æµ‹åˆ° GPU è®¾å¤‡çš„æ‰€å±ç”¨æˆ·ç»„ï¼Œå®¹å™¨å¯èƒ½æ— æ³•è®¿é—® GPUã€‚", file=sys.stderr)
    else:
        for gid in gpu_device_groups:
            docker_command.extend(["--group-add", str(gid)])
        print(
            f"ğŸ” ä¸ºå®¹å™¨æ·»åŠ  GPU ç›¸å…³ç”¨æˆ·ç»„: {', '.join(str(g) for g in gpu_device_groups)}",
            file=sys.stderr,
        )

    docker_command.extend(extra_args)

    docker_command.append(image)
    docker_command.extend(
        [
            "python",
            "run_alphafold.py",
            f"--json_path={container_input_dir}/fold_input.json",
            f"--model_dir={container_model_dir}",
            f"--output_dir={container_output_dir}",
            f"--db_dir={container_database_dir}",
        ]
    )

    display_command = " ".join(shlex.quote(part) for part in docker_command)
    print(f"ğŸ³ è¿è¡Œ AlphaFold3 Docker: {display_command}", file=sys.stderr)
    docker_proc = subprocess.run(
        docker_command,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
    )

    if docker_proc.returncode != 0:
        print(f"âŒ AlphaFold3 Docker è¿è¡Œå¤±è´¥: {docker_proc.stderr}", file=sys.stderr)
        raise RuntimeError(
            f"AlphaFold3 Docker run failed with exit code {docker_proc.returncode}. "
            f"Stdout: {docker_proc.stdout}\nStderr: {docker_proc.stderr}"
        )

    print(f"âœ… AlphaFold3 Docker è¿è¡Œå®Œæˆ: {docker_proc.stdout}", file=sys.stderr)

    af3_output_contents = list(Path(af3_output_dir).rglob("*"))
    if not any(p.is_file() for p in af3_output_contents):
        print("âš ï¸ AlphaFold3 è¾“å‡ºç›®å½•ä¸ºç©ºï¼Œå¯èƒ½æ¨ç†æœªäº§ç”Ÿç»“æœã€‚", file=sys.stderr)

    extra_archive_files = run_af3_affinity_pipeline(
        temp_dir=temp_dir,
        yaml_data=yaml_data,
        prep=prep,
        af3_output_dir=af3_output_dir,
    )

    create_af3_archive(
        output_archive_path,
        fasta_content,
        af3_json,
        chain_msa_paths,
        yaml_content,
        prep,
        af3_output_dir=af3_output_dir,
        extra_files=extra_archive_files,
    )

def main():
    """
    Main function to run a single prediction based on arguments provided in a JSON file.
    The JSON file should contain the necessary parameters for the prediction, including:
    - output_archive_path: Path where the output archive will be saved.
    - yaml_content: YAML content as a string that will be written to a temporary file.
    - Other parameters that will be passed to the predict function as command-line arguments.
    """
    if len(sys.argv) != 2:
        print("Usage: python run_single_prediction.py <args_file_path>")
        sys.exit(1)

    args_file_path = sys.argv[1]

    try:
        with open(args_file_path, 'r') as f:
            predict_args = json.load(f)

        output_archive_path = predict_args.pop("output_archive_path")
        yaml_content = predict_args.pop("yaml_content")
        backend = str(predict_args.pop("backend", "boltz")).strip().lower()
        if backend not in ("boltz", "alphafold3"):
            raise ValueError(f"Unsupported backend '{backend}'.")

        model_name = predict_args.pop("model_name", None)

        use_msa_server = predict_args.get("use_msa_server", False)

        with tempfile.TemporaryDirectory() as temp_dir:
            if backend == "alphafold3":
                run_alphafold3_backend(temp_dir, yaml_content, output_archive_path, use_msa_server)
            else:
                run_boltz_backend(temp_dir, yaml_content, output_archive_path, predict_args, model_name)

            if not os.path.exists(output_archive_path):
                raise FileNotFoundError(
                    f"CRITICAL ERROR: Archive not found at {output_archive_path} immediately after creation."
                )

            print(f"DEBUG: Archive successfully created at: {output_archive_path}", file=sys.stderr)

    except Exception as e:
        print(f"Error during prediction subprocess: {e}\n{traceback.format_exc()}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()
