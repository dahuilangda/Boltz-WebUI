# run_single_prediction.py
import sys
import os
import json
import tempfile
import shutil
import traceback
import yaml
import hashlib
import glob
import csv
import zipfile
from pathlib import Path

sys.path.append(os.getcwd())
from boltz_wrapper import predict

# MSA ç¼“å­˜é…ç½®
MSA_CACHE_CONFIG = {
    'cache_dir': '/tmp/boltz_msa_cache',
    'enable_cache': True
}

def get_sequence_hash(sequence: str) -> str:
    """è®¡ç®—åºåˆ—çš„MD5å“ˆå¸Œå€¼ä½œä¸ºç¼“å­˜é”®"""
    return hashlib.md5(sequence.encode('utf-8')).hexdigest()

def cache_msa_files_from_temp_dir(temp_dir: str, yaml_content: str):
    """
    ä»ä¸´æ—¶ç›®å½•ä¸­ç¼“å­˜ç”Ÿæˆçš„MSAæ–‡ä»¶
    æ”¯æŒä»colabfold serverç”Ÿæˆçš„CSVæ ¼å¼MSAæ–‡ä»¶
    ä¸ºæ¯ä¸ªè›‹ç™½è´¨ç»„åˆ†å•ç‹¬ç¼“å­˜MSAï¼Œé€‚ç”¨äºç»“æ„é¢„æµ‹å’Œåˆ†å­è®¾è®¡
    """
    if not MSA_CACHE_CONFIG['enable_cache']:
        return
    
    try:
        # è§£æYAMLè·å–è›‹ç™½è´¨åºåˆ—
        yaml_data = yaml.safe_load(yaml_content)
        protein_sequences = {}
        
        # æå–æ‰€æœ‰è›‹ç™½è´¨åºåˆ—ï¼ˆæ”¯æŒç»“æ„é¢„æµ‹å’Œåˆ†å­è®¾è®¡ï¼‰
        for entity in yaml_data.get('sequences', []):
            if entity.get('protein', {}).get('id'):
                protein_id = entity['protein']['id']
                sequence = entity['protein'].get('sequence', '')
                if sequence:
                    protein_sequences[protein_id] = sequence
        
        if not protein_sequences:
            print("æœªæ‰¾åˆ°è›‹ç™½è´¨åºåˆ—ï¼Œè·³è¿‡MSAç¼“å­˜", file=sys.stderr)
            return
        
        print(f"éœ€è¦ç¼“å­˜çš„è›‹ç™½è´¨ç»„åˆ†: {list(protein_sequences.keys())}", file=sys.stderr)
        
        # è®¾ç½®ç¼“å­˜ç›®å½•
        cache_dir = MSA_CACHE_CONFIG['cache_dir']
        os.makedirs(cache_dir, exist_ok=True)
        
        # é€’å½’æœç´¢ä¸´æ—¶ç›®å½•ä¸­çš„MSAæ–‡ä»¶
        print(f"é€’å½’æœç´¢ä¸´æ—¶ç›®å½•ä¸­çš„MSAæ–‡ä»¶: {temp_dir}", file=sys.stderr)
        
        # ä¸ºæ¯ä¸ªè›‹ç™½è´¨ç»„åˆ†å•ç‹¬æŸ¥æ‰¾å¯¹åº”çš„MSAæ–‡ä»¶
        protein_msa_map = {}  # protein_id -> [msa_files]
        
        # æœç´¢æ‰€æœ‰MSAæ–‡ä»¶
        all_msa_files = []
        for root, dirs, files in os.walk(temp_dir):
            for file in files:
                if file.endswith('.csv') or file.endswith('.a3m'):
                    file_path = os.path.join(root, file)
                    all_msa_files.append(file_path)
        
        if not all_msa_files:
            print(f"åœ¨ä¸´æ—¶ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•MSAæ–‡ä»¶: {temp_dir}", file=sys.stderr)
            return
        
        print(f"æ‰¾åˆ° {len(all_msa_files)} ä¸ªMSAæ–‡ä»¶: {[os.path.basename(f) for f in all_msa_files]}", file=sys.stderr)
        
        # ä¸ºæ¯ä¸ªè›‹ç™½è´¨ç»„åˆ†åŒ¹é…å¯¹åº”çš„MSAæ–‡ä»¶
        for protein_id in protein_sequences.keys():
            protein_msa_map[protein_id] = []
            
            for msa_file in all_msa_files:
                filename = os.path.basename(msa_file)
                
                # ç²¾ç¡®åŒ¹é…ï¼šæ–‡ä»¶ååŒ…å«protein ID
                if protein_id.lower() in filename.lower():
                    protein_msa_map[protein_id].append(msa_file)
                    continue
                    
                # ç´¢å¼•åŒ¹é…ï¼šå¦‚æœprotein_idæ˜¯å­—æ¯ï¼Œå°è¯•åŒ¹é…å¯¹åº”çš„æ•°å­—ç´¢å¼•
                # ä¾‹å¦‚ï¼šprotein A -> _0.csv, protein B -> _1.csv
                if len(protein_id) == 1 and protein_id.isalpha():
                    protein_index = ord(protein_id.upper()) - ord('A')
                    if f"_{protein_index}." in filename:
                        protein_msa_map[protein_id].append(msa_file)
                        continue
                
                # é€šç”¨åŒ¹é…ï¼šå¦‚æœåªæœ‰ä¸€ä¸ªè›‹ç™½è´¨ç»„åˆ†ï¼Œä½¿ç”¨é€šç”¨MSAæ–‡ä»¶
                if len(protein_sequences) == 1 and any(pattern in filename.lower() for pattern in ['msa', '_0.csv', '_0.a3m']):
                    protein_msa_map[protein_id].append(msa_file)
        
        # å¤„ç†æ¯ä¸ªè›‹ç™½è´¨ç»„åˆ†çš„MSAæ–‡ä»¶
        cached_count = 0
        for protein_id, msa_files in protein_msa_map.items():
            if not msa_files:
                print(f"âŒ è›‹ç™½è´¨ç»„åˆ† {protein_id} æœªæ‰¾åˆ°å¯¹åº”çš„MSAæ–‡ä»¶", file=sys.stderr)
                continue
                
            print(f"ğŸ” å¤„ç†è›‹ç™½è´¨ç»„åˆ† {protein_id} çš„ {len(msa_files)} ä¸ªMSAæ–‡ä»¶", file=sys.stderr)
            
            for msa_file in msa_files:
                if cache_single_protein_msa(protein_id, protein_sequences[protein_id], msa_file, cache_dir):
                    cached_count += 1
                    break  # æˆåŠŸç¼“å­˜ä¸€ä¸ªå°±å¤Ÿäº†
        
        print(f"âœ… MSAç¼“å­˜å®Œæˆï¼ŒæˆåŠŸç¼“å­˜ {cached_count}/{len(protein_sequences)} ä¸ªè›‹ç™½è´¨ç»„åˆ†", file=sys.stderr)
                
    except Exception as e:
        print(f"âŒ ç¼“å­˜MSAæ–‡ä»¶å¤±è´¥: {e}", file=sys.stderr)

def cache_single_protein_msa(protein_id: str, protein_sequence: str, msa_file: str, cache_dir: str) -> bool:
    """
    ä¸ºå•ä¸ªè›‹ç™½è´¨ç»„åˆ†ç¼“å­˜MSAæ–‡ä»¶
    è¿”å›æ˜¯å¦æˆåŠŸç¼“å­˜
    """
    try:
        filename = os.path.basename(msa_file)
        file_ext = os.path.splitext(filename)[1].lower()
        
        print(f"  ğŸ“‚ å¤„ç†MSAæ–‡ä»¶: {filename}", file=sys.stderr)
        
        if file_ext == '.csv':
            # å¤„ç†CSVæ ¼å¼çš„MSAæ–‡ä»¶ï¼ˆæ¥è‡ªcolabfold serverï¼‰
            with open(msa_file, 'r') as f:
                reader = csv.reader(f)
                header = next(reader, None)
                if header and len(header) >= 2 and 'sequence' in header:
                    sequences = []
                    for row in reader:
                        if len(row) >= 2 and row[1]:
                            sequences.append(row[1])
                    
                    if sequences:
                        # ç¬¬ä¸€ä¸ªåºåˆ—é€šå¸¸æ˜¯æŸ¥è¯¢åºåˆ—
                        query_sequence = sequences[0]
                        print(f"    ä»CSVæå–çš„æŸ¥è¯¢åºåˆ—: {query_sequence[:50]}...", file=sys.stderr)
                        
                        # éªŒè¯åºåˆ—æ˜¯å¦åŒ¹é…
                        if is_sequence_match(protein_sequence, query_sequence):
                            # è½¬æ¢CSVæ ¼å¼åˆ°A3Mæ ¼å¼
                            a3m_content = f">{protein_id}\n{query_sequence}\n"
                            for i, seq in enumerate(sequences[1:], 1):
                                a3m_content += f">seq_{i}\n{seq}\n"
                            
                            # ç¼“å­˜è½¬æ¢åçš„A3Mæ–‡ä»¶
                            seq_hash = get_sequence_hash(protein_sequence)
                            cache_path = os.path.join(cache_dir, f"msa_{seq_hash}.a3m")
                            with open(cache_path, 'w') as cache_file:
                                cache_file.write(a3m_content)
                            print(f"    âœ… æˆåŠŸç¼“å­˜è›‹ç™½è´¨ç»„åˆ† {protein_id} çš„MSA (ä»CSVè½¬æ¢): {cache_path}", file=sys.stderr)
                            print(f"       åºåˆ—å“ˆå¸Œ: {seq_hash}", file=sys.stderr)
                            print(f"       MSAåºåˆ—æ•°: {len(sequences)}", file=sys.stderr)
                            return True
                        else:
                            print(f"    âŒ CSVæ–‡ä»¶ä¸­çš„æŸ¥è¯¢åºåˆ—ä¸è›‹ç™½è´¨ç»„åˆ† {protein_id} ä¸åŒ¹é…", file=sys.stderr)
                            return False
        
        elif file_ext == '.a3m':
            # å¤„ç†A3Mæ ¼å¼çš„MSAæ–‡ä»¶
            with open(msa_file, 'r') as f:
                msa_content = f.read()
            
            # ä»MSAå†…å®¹ä¸­æå–æŸ¥è¯¢åºåˆ—ï¼ˆç¬¬ä¸€ä¸ªåºåˆ—ï¼‰
            lines = msa_content.strip().split('\n')
            if len(lines) >= 2 and lines[0].startswith('>'):
                query_sequence = lines[1]
                
                # éªŒè¯åºåˆ—æ˜¯å¦åŒ¹é…
                if is_sequence_match(protein_sequence, query_sequence):
                    # ç¼“å­˜MSAæ–‡ä»¶
                    seq_hash = get_sequence_hash(protein_sequence)
                    cache_path = os.path.join(cache_dir, f"msa_{seq_hash}.a3m")
                    shutil.copy2(msa_file, cache_path)
                    print(f"    âœ… æˆåŠŸç¼“å­˜è›‹ç™½è´¨ç»„åˆ† {protein_id} çš„MSA: {cache_path}", file=sys.stderr)
                    print(f"       åºåˆ—å“ˆå¸Œ: {seq_hash}", file=sys.stderr)
                    return True
                else:
                    print(f"    âŒ A3Mæ–‡ä»¶ä¸­çš„æŸ¥è¯¢åºåˆ—ä¸è›‹ç™½è´¨ç»„åˆ† {protein_id} ä¸åŒ¹é…", file=sys.stderr)
                    return False
        
        return False
        
    except Exception as e:
        print(f"    âŒ å¤„ç†è›‹ç™½è´¨ç»„åˆ† {protein_id} çš„MSAæ–‡ä»¶å¤±è´¥ {msa_file}: {e}", file=sys.stderr)
        return False

def is_sequence_match(protein_sequence: str, query_sequence: str) -> bool:
    """
    æ£€æŸ¥è›‹ç™½è´¨åºåˆ—å’ŒæŸ¥è¯¢åºåˆ—æ˜¯å¦åŒ¹é…
    æ”¯æŒå®Œå…¨åŒ¹é…ã€å®¹é”™åŒ¹é…å’Œç›¸ä¼¼åº¦åŒ¹é…
    """
    # å®Œå…¨åŒ¹é…
    if protein_sequence == query_sequence:
        return True
    
    # å®¹é”™åŒ¹é…ï¼šå»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦åæ¯”è¾ƒ
    clean_protein = protein_sequence.replace('-', '').replace(' ', '').upper()
    clean_query = query_sequence.replace('-', '').replace(' ', '').upper()
    if clean_protein == clean_query:
        return True
    
    # å­åºåˆ—åŒ¹é…ï¼šæŸ¥è¯¢åºåˆ—å¯èƒ½æ˜¯è›‹ç™½è´¨åºåˆ—çš„ä¸€éƒ¨åˆ†
    if clean_query in clean_protein or clean_protein in clean_query:
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = len(set(clean_query) & set(clean_protein)) / max(len(clean_query), len(clean_protein))
        if similarity > 0.8:  # 80%ç›¸ä¼¼åº¦é˜ˆå€¼
            return True
    
    return False

def find_results_dir(base_dir: str) -> str:
    result_path = None
    max_depth = -1
    for root, dirs, files in os.walk(base_dir):
        if any(f.endswith((".cif")) for f in files):
            depth = root.count(os.sep)
            if depth > max_depth:
                max_depth = depth
                result_path = root

    if result_path:
        print(f"Found results in directory: {result_path}", file=sys.stderr)
        return result_path

    raise FileNotFoundError(f"Could not find any directory containing result files within the base directory {base_dir}")

def get_cached_a3m_files(yaml_content: str) -> list:
    """
    è·å–ä¸å½“å‰é¢„æµ‹ä»»åŠ¡ç›¸å…³çš„a3mç¼“å­˜æ–‡ä»¶
    è¿”å›ç¼“å­˜æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    cached_a3m_files = []
    
    if not MSA_CACHE_CONFIG['enable_cache']:
        return cached_a3m_files
    
    try:
        # è§£æYAMLè·å–è›‹ç™½è´¨åºåˆ—
        yaml_data = yaml.safe_load(yaml_content)
        protein_sequences = {}
        
        # æå–æ‰€æœ‰è›‹ç™½è´¨åºåˆ—
        for entity in yaml_data.get('sequences', []):
            if entity.get('protein', {}).get('id'):
                protein_id = entity['protein']['id']
                sequence = entity['protein'].get('sequence', '')
                if sequence:
                    protein_sequences[protein_id] = sequence
        
        if not protein_sequences:
            print("æœªæ‰¾åˆ°è›‹ç™½è´¨åºåˆ—ï¼Œè·³è¿‡a3mæ–‡ä»¶æ”¶é›†", file=sys.stderr)
            return cached_a3m_files
        
        cache_dir = MSA_CACHE_CONFIG['cache_dir']
        if not os.path.exists(cache_dir):
            return cached_a3m_files
        
        print(f"æŸ¥æ‰¾ç¼“å­˜çš„a3mæ–‡ä»¶ï¼Œè›‹ç™½è´¨ç»„åˆ†: {list(protein_sequences.keys())}", file=sys.stderr)
        
        # ä¸ºæ¯ä¸ªè›‹ç™½è´¨åºåˆ—æŸ¥æ‰¾å¯¹åº”çš„ç¼“å­˜æ–‡ä»¶
        for protein_id, sequence in protein_sequences.items():
            seq_hash = get_sequence_hash(sequence)
            cache_file_path = os.path.join(cache_dir, f"msa_{seq_hash}.a3m")
            
            if os.path.exists(cache_file_path):
                cached_a3m_files.append({
                    'path': cache_file_path,
                    'protein_id': protein_id,
                    'filename': f"{protein_id}_msa.a3m"
                })
                print(f"æ‰¾åˆ°ç¼“å­˜æ–‡ä»¶: {protein_id} -> {cache_file_path}", file=sys.stderr)
        
        print(f"æ€»å…±æ‰¾åˆ° {len(cached_a3m_files)} ä¸ªa3mç¼“å­˜æ–‡ä»¶", file=sys.stderr)
        
    except Exception as e:
        print(f"è·å–a3mç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}", file=sys.stderr)
    
    return cached_a3m_files

def create_archive_with_a3m(output_archive_path: str, output_directory_path: str, yaml_content: str):
    """
    åˆ›å»ºåŒ…å«é¢„æµ‹ç»“æœå’Œa3mç¼“å­˜æ–‡ä»¶çš„zipå½’æ¡£
    """
    try:
        # è·å–ç›¸å…³çš„a3mç¼“å­˜æ–‡ä»¶
        cached_a3m_files = get_cached_a3m_files(yaml_content)
        
        # åˆ›å»ºzipæ–‡ä»¶
        with zipfile.ZipFile(output_archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            # æ·»åŠ é¢„æµ‹ç»“æœæ–‡ä»¶
            for root, dirs, files in os.walk(output_directory_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œä¿æŒç›®å½•ç»“æ„
                    arcname = os.path.relpath(file_path, output_directory_path)
                    zipf.write(file_path, arcname)
                    print(f"æ·»åŠ ç»“æœæ–‡ä»¶: {arcname}", file=sys.stderr)
            
            # æ·»åŠ a3mç¼“å­˜æ–‡ä»¶
            if cached_a3m_files:
                # åœ¨zipä¸­åˆ›å»ºmsaç›®å½•
                for a3m_info in cached_a3m_files:
                    cache_file_path = a3m_info['path']
                    filename = a3m_info['filename']
                    # å°†a3mæ–‡ä»¶æ”¾åœ¨msaå­ç›®å½•ä¸­
                    arcname = f"msa/{filename}"
                    zipf.write(cache_file_path, arcname)
                    print(f"æ·»åŠ a3mç¼“å­˜æ–‡ä»¶: {arcname}", file=sys.stderr)
                
                print(f"âœ… æˆåŠŸæ·»åŠ  {len(cached_a3m_files)} ä¸ªa3mç¼“å­˜æ–‡ä»¶åˆ°zipå½’æ¡£", file=sys.stderr)
            else:
                print("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³çš„a3mç¼“å­˜æ–‡ä»¶", file=sys.stderr)
        
        print(f"âœ… å½’æ¡£åˆ›å»ºå®Œæˆ: {output_archive_path}", file=sys.stderr)
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºåŒ…å«a3mæ–‡ä»¶çš„å½’æ¡£å¤±è´¥: {e}", file=sys.stderr)
        # å¦‚æœå¤±è´¥ï¼Œå›é€€åˆ°åŸæ¥çš„æ–¹å¼
        archive_base_name = output_archive_path.rsplit('.', 1)[0]
        created_archive_path = shutil.make_archive(
            base_name=archive_base_name,
            format='zip',
            root_dir=output_directory_path
        )
        print(f"å›é€€åˆ°æ ‡å‡†å½’æ¡£æ–¹å¼: {created_archive_path}", file=sys.stderr)

def main():
    """
    Main function to run a single prediction based on arguments provided in a JSON file.
    The JSON file should contain the necessary parameters for the prediction, including:
    - output_archive_path: Path where the output archive will be saved.
    - yaml_content: YAML content as a string that will be written to a temporary file.
    - Other parameters that will be passed to the predict function as command-line arguments.
    """
    if len(sys.argv) != 2:
        print("Usage: python run_single_prediction.py <args_file_path>")
        sys.exit(1)

    args_file_path = sys.argv[1]

    try:
        with open(args_file_path, 'r') as f:
            predict_args = json.load(f)

        output_archive_path = predict_args.pop("output_archive_path")

        with tempfile.TemporaryDirectory() as temp_dir:
            yaml_content = predict_args.pop("yaml_content")

            tmp_yaml_path = os.path.join(temp_dir, 'data.yaml')
            with open(tmp_yaml_path, 'w') as tmp_yaml:
                tmp_yaml.write(yaml_content)

            predict_args['data'] = tmp_yaml_path
            predict_args['out_dir'] = temp_dir
            
            POSITIONAL_KEYS = ['data']
            
            cmd_positional = []
            cmd_options = []

            for key, value in predict_args.items():
                if key in POSITIONAL_KEYS:
                    cmd_positional.append(str(value))
                else:
                    if value is None:
                        continue
                    if isinstance(value, bool):
                        if value:
                            cmd_options.append(f'--{key}')
                    else:
                        cmd_options.append(f'--{key}')
                        cmd_options.append(str(value))

            cmd_args = cmd_positional + cmd_options

            print(f"DEBUG: Invoking predict with args: {cmd_args}", file=sys.stderr)
            predict.main(args=cmd_args, standalone_mode=False)

            # åœ¨é¢„æµ‹å®Œæˆåç«‹å³ç¼“å­˜MSAæ–‡ä»¶
            cache_msa_files_from_temp_dir(temp_dir, yaml_content)

            output_directory_path = find_results_dir(temp_dir)

            if not os.listdir(output_directory_path):
                raise NotADirectoryError(f"Prediction result directory was found but is empty: {output_directory_path}")

            # ä½¿ç”¨æ–°çš„å‡½æ•°åˆ›å»ºåŒ…å«a3mæ–‡ä»¶çš„å½’æ¡£
            create_archive_with_a3m(output_archive_path, output_directory_path, yaml_content)

            if not os.path.exists(output_archive_path):
                raise FileNotFoundError(f"CRITICAL ERROR: Archive not found at {output_archive_path} immediately after creation.")
            
            print(f"DEBUG: Archive successfully created at: {output_archive_path}", file=sys.stderr)

    except Exception as e:
        print(f"Error during prediction subprocess: {e}\n{traceback.format_exc()}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()